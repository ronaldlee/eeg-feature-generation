{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f620c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d3f5823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (4284224, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_df = pd.read_csv('./csv/out_gameemo_time_domain_simple.csv',  sep=',')\n",
    "\n",
    "print('Shape of data: ', init_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5f28f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AF3      AF4       F3       F4       F7       F8      FC5      FC6  \\\n",
      "0 -11.5692 -7.71280  13.0154 -7.71280  76.1643   9.1590  38.0819 -0.48193   \n",
      "1 -12.5625 -6.52730  12.9572 -6.04490  76.1735  11.7427  35.5558 -0.69421   \n",
      "2 -14.7008 -6.13520  12.6621 -4.71790  75.4600  13.9307  31.4947 -1.13450   \n",
      "3  -7.3113 -2.63410  12.6253 -1.78380  83.2244  21.5305  29.8460  1.10270   \n",
      "4  -2.2931 -0.78859  11.6270 -0.47166  89.5589  26.7464  26.8497  1.75970   \n",
      "\n",
      "        O1      O2       P7       P8       T7       T8  Label  \n",
      "0 -1.44630 -5.3026  0.48193 -10.1229  8.67710  5.30260    2.0  \n",
      "1 -1.11830 -5.7076  0.69421  -9.7567  7.91530  3.77900    2.0  \n",
      "2 -0.56926 -7.2934  0.65256 -11.0995  5.51220  0.17823    2.0  \n",
      "3 -2.22230 -7.0967 -1.07380  -8.2644  3.49420  1.85480    2.0  \n",
      "4 -5.70420 -7.8760 -4.62500  -7.0454  0.63323  1.98420    2.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = init_df.copy()\n",
    "print(df.head())\n",
    "\n",
    "#HA_PV = high arousal, positive valence\n",
    "#HA_NV = high arousal, negative valence\n",
    "#LA_NV = low arousal, negative valence\n",
    "#LA_PV = low arousal, positive valance\n",
    "label_map = {1:\"HA_PV\", 2:\"HA_NV\", 3:\"LA_NV\", 4:\"LA_PV\"}\n",
    "\n",
    "df[\"Label\"] = df[\"Label\"].map(label_map)\n",
    "\n",
    "# df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e597b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure the X features data set to group them by samples.\n",
    "# We know the sample size is 38252 each, so we just need to iterate and group them\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d9eb7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AF3      AF4       F3       F4       F7       F8      FC5      FC6  \\\n",
      "0 -11.5692 -7.71280  13.0154 -7.71280  76.1643   9.1590  38.0819 -0.48193   \n",
      "1 -12.5625 -6.52730  12.9572 -6.04490  76.1735  11.7427  35.5558 -0.69421   \n",
      "2 -14.7008 -6.13520  12.6621 -4.71790  75.4600  13.9307  31.4947 -1.13450   \n",
      "3  -7.3113 -2.63410  12.6253 -1.78380  83.2244  21.5305  29.8460  1.10270   \n",
      "4  -2.2931 -0.78859  11.6270 -0.47166  89.5589  26.7464  26.8497  1.75970   \n",
      "\n",
      "        O1      O2       P7       P8       T7       T8  Label  \n",
      "0 -1.44630 -5.3026  0.48193 -10.1229  8.67710  5.30260  HA_NV  \n",
      "1 -1.11830 -5.7076  0.69421  -9.7567  7.91530  3.77900  HA_NV  \n",
      "2 -0.56926 -7.2934  0.65256 -11.0995  5.51220  0.17823  HA_NV  \n",
      "3 -2.22230 -7.0967 -1.07380  -8.2644  3.49420  1.85480  HA_NV  \n",
      "4 -5.70420 -7.8760 -4.62500  -7.0454  0.63323  1.98420  HA_NV  \n",
      "Shape of data:  (4284224, 15)\n",
      "features.shape:  (4284224, 14)\n",
      "label.shape:  (4284224, 1)\n",
      "Index(['AF3', 'AF4', 'F3', 'F4', 'F7', 'F8', 'FC5', 'FC6', 'O1', 'O2', 'P7',\n",
      "       'P8', 'T7', 'T8', 'Label'],\n",
      "      dtype='object')\n",
      "total_samples_count: 112\n",
      "train size: 2983656\n",
      "test size: 1300568\n",
      "X_train.shape after reshape: (78, 38252, 14)\n",
      "X_test.shape after reshape: (34, 38252, 14)\n",
      "y_train_collapsed shape: (78,)\n",
      "y_test_collapsed shape: (34,)\n",
      "y_train.shape: (78, 4)\n",
      "y_test.shape: (34, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "features = df.iloc[:, :-1]\n",
    "label = df.iloc[:, -1:]\n",
    "\n",
    "print('Shape of data: ', df.shape)\n",
    "print('features.shape: ', features.shape)\n",
    "print('label.shape: ', label.shape)\n",
    "\n",
    "df.head()\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "y = label\n",
    "X = features\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=48)\n",
    "\n",
    "total_samples_count = int(X.shape[0]/38252)\n",
    "\n",
    "print(\"total_samples_count:\", total_samples_count)\n",
    "\n",
    "\n",
    "train_sample_count = int(total_samples_count * 0.7)\n",
    "test_sample_count = total_samples_count - train_sample_count\n",
    "\n",
    "train_size = train_sample_count * 38252\n",
    "test_size = test_sample_count * 38252\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"test size:\", test_size)\n",
    "\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "\n",
    "X_train = np.array(X_train).reshape((train_sample_count,38252,14))\n",
    "X_test = np.array(X_test).reshape((test_sample_count,38252,14))\n",
    "\n",
    "print(\"X_train.shape after reshape:\",X_train.shape)\n",
    "print(\"X_test.shape after reshape:\",X_test.shape)\n",
    "\n",
    "#collapse y_train and y_test to the same X sample counts instead\n",
    "\n",
    "y_train_collapsed = np.array([])\n",
    "for i in range(len(y_train)):\n",
    "    if (i % 38252 == 0):\n",
    "        y_train_collapsed = np.append(y_train_collapsed, (y_train.iloc[i]))\n",
    "        \n",
    "print(\"y_train_collapsed shape:\",y_train_collapsed.shape)        \n",
    "\n",
    "y_test_collapsed = np.array([])\n",
    "for i in range(len(y_test)):\n",
    "    if (i % 38252 == 0):\n",
    "        y_test_collapsed = np.append(y_test_collapsed, (y_test.iloc[i]))\n",
    "        \n",
    "print(\"y_test_collapsed shape:\",y_test_collapsed.shape)    \n",
    "\n",
    "\n",
    "y_train = pd.get_dummies(y_train_collapsed)\n",
    "y_test = pd.get_dummies(y_test_collapsed)\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c2fa0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train,x_test,y_test, save_to, epoch = 2):\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    with strategy.scope():\n",
    "        #inputs = tf.keras.Input(shape=(X_train.shape[0],14)) #input_dim = 14 channels(features)\n",
    "#         inputs = tf.keras.Input(shape=(38252,14)) #input_dim = 14 channels(features)\n",
    "        inputs = tf.keras.Input(shape=(38252,14))\n",
    "        \n",
    "\n",
    "        #ml_model = tf.keras.layers.GRU(256, return_sequences=True)(inputs)\n",
    "        ml_model = tf.keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
    "\n",
    "        flat = Flatten()(ml_model)\n",
    "        outputs = Dense(4, activation='softmax')(flat)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        #model = tf.keras.models.load_model('_best_model.h5')\n",
    "\n",
    "        model.summary()\n",
    "        tf.keras.utils.plot_model(model)\n",
    "\n",
    "        opt_adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + '_best_model_lstm_time_domain.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "            \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "            \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                      loss=['categorical_crossentropy'],\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "          \n",
    "    history = model.fit(x_train,y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(x_test,y_test),\n",
    "                        callbacks=[es,mc,lr_schedule], shuffle=False)\n",
    "        \n",
    "    # saved_model = load_model(save_to + '_best_model_lstm_all_cat.h5')\n",
    "        \n",
    "    return model,history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "913c157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        [(None, 38252, 14)]       0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 38252, 256)        277504    \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 9792512)           0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 4)                 39170052  \n",
      "=================================================================\n",
      "Total params: 39,447,556\n",
      "Trainable params: 39,447,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "RRRRRR right before: x_train.shape:\n",
      "[[[-1.156920e+01 -7.712800e+00  1.301540e+01 ... -1.012290e+01\n",
      "    8.677100e+00  5.302600e+00]\n",
      "  [-1.256250e+01 -6.527300e+00  1.295720e+01 ... -9.756700e+00\n",
      "    7.915300e+00  3.779000e+00]\n",
      "  [-1.470080e+01 -6.135200e+00  1.266210e+01 ... -1.109950e+01\n",
      "    5.512200e+00  1.782300e-01]\n",
      "  ...\n",
      "  [-2.359000e+00  3.077500e+00 -3.468000e+00 ... -6.103400e+00\n",
      "    4.125300e+00 -2.872700e+00]\n",
      "  [-1.976500e+00 -1.204500e+00 -2.054600e+00 ... -2.603700e+00\n",
      "    1.226700e+00  6.217700e+00]\n",
      "  [-3.304200e+00 -8.363000e+00  4.335200e+00 ... -1.483600e+00\n",
      "   -1.739000e+00  2.470200e+00]]\n",
      "\n",
      " [[-9.882200e+00 -4.097600e+00 -1.205100e+00 ...  1.373810e+01\n",
      "    7.231300e-01  1.566670e+01]\n",
      "  [-1.338660e+01 -5.539000e+00 -8.917900e-01 ...  9.298800e+00\n",
      "    9.207100e-01  1.207520e+01]\n",
      "  [-2.911000e-01  2.747400e+00 -1.151600e-01 ...  8.017800e+00\n",
      "   -8.052400e+00  9.865800e-01]\n",
      "  ...\n",
      "  [-1.499300e+00 -3.651500e+00 -1.706100e-01 ... -2.280500e-01\n",
      "   -1.632200e+00  3.418800e+00]\n",
      "  [-3.578500e+00 -3.191500e+00 -4.013400e-01 ...  2.659800e-02\n",
      "   -3.703400e+00  1.044600e+00]\n",
      "  [-2.641100e+00 -1.794900e+00 -9.295100e+00 ...  3.640200e+00\n",
      "   -1.311800e+00 -8.418400e+00]]\n",
      "\n",
      " [[ 9.158800e+00 -4.820500e+00  2.072840e+01 ...  4.338500e+00\n",
      "   -4.338500e+00  1.060510e+01]\n",
      "  [ 7.645400e+00 -6.941400e+00  1.177190e+01 ...  4.560200e+00\n",
      "   -4.560200e+00  1.430750e+01]\n",
      "  [ 1.160900e+00 -9.658600e+00  1.564470e+01 ...  7.419800e+00\n",
      "   -1.635200e+00  5.495100e+00]\n",
      "  ...\n",
      "  [ 4.962400e+00  1.103900e+00 -4.664700e+00 ...  1.358200e+00\n",
      "   -5.082500e+00  2.256300e+00]\n",
      "  [-3.771200e+00 -4.023800e+00 -3.661700e+00 ...  8.748500e+00\n",
      "   -1.162200e+00  2.844100e+00]\n",
      "  [-1.294510e+01 -4.505300e+00 -9.467600e+00 ...  4.126200e+00\n",
      "    3.969100e+00 -2.388100e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-5.884220e+01  2.000631e+02 -5.319380e+01 ... -1.379266e+02\n",
      "    5.177700e+00  6.260810e+01]\n",
      "  [-4.121170e+01  1.861764e+02 -5.282640e+01 ... -1.437510e+02\n",
      "    2.984100e+00  6.073450e+01]\n",
      "  [-2.789010e+01  1.707693e+02 -3.555680e+01 ... -1.492259e+02\n",
      "    5.159100e+00  6.791750e+01]\n",
      "  ...\n",
      "  [-1.747330e+01 -5.699290e+01  6.676500e-01 ...  1.717530e+01\n",
      "   -7.874400e+00  6.699000e+00]\n",
      "  [-2.324900e+00 -5.498610e+01 -5.492300e+00 ...  2.044800e+00\n",
      "    5.307600e+00 -7.803000e+00]\n",
      "  [ 4.404600e+00 -5.121580e+01  2.840100e+00 ...  2.818800e+00\n",
      "    1.487440e+01  2.057600e+00]]\n",
      "\n",
      " [[ 9.648700e+00 -6.753900e+00  1.543840e+01 ...  2.026250e+01\n",
      "   -1.128904e+02  4.824600e+00]\n",
      "  [ 3.280600e+00 -1.310340e+01  1.065210e+01 ...  1.036270e+01\n",
      "   -9.453860e+01  8.394100e+00]\n",
      "  [ 8.390200e+00 -3.150400e+00  1.628500e+01 ...  7.328400e+00\n",
      "   -7.487530e+01 -6.209500e+00]\n",
      "  ...\n",
      "  [-9.093700e+00 -4.613000e+00 -2.458690e+01 ... -2.269760e+01\n",
      "    5.731610e+01  1.498660e+01]\n",
      "  [ 3.030800e+00 -2.407400e+00 -9.011700e+00 ... -7.828500e+00\n",
      "    5.677150e+01 -1.261400e-02]\n",
      "  [ 1.884300e+00 -3.327400e-01  5.629000e+00 ... -6.044600e-01\n",
      "    4.950570e+01 -6.174300e+00]]\n",
      "\n",
      " [[-2.408887e+02  5.358290e+01  7.920800e+00 ...  2.842230e+01\n",
      "   -1.630730e+01  5.731030e+01]\n",
      "  [-2.189809e+02  4.850330e+01  4.650100e+00 ...  3.323980e+01\n",
      "   -1.532930e+01  4.734800e+01]\n",
      "  [-1.918638e+02  4.652610e+01  9.030700e+00 ...  3.963240e+01\n",
      "   -1.347750e+01  5.662160e+01]\n",
      "  ...\n",
      "  [-1.357470e+01  3.173600e+00 -2.022000e+00 ...  6.529200e+00\n",
      "    2.113500e+01  6.360400e+00]\n",
      "  [-1.741930e+01  2.051500e+00 -1.900700e+00 ...  5.467200e-01\n",
      "    5.766900e+00  4.115500e+00]\n",
      "  [-2.150000e+01  1.462600e+00 -3.184100e+00 ... -6.476100e+00\n",
      "   -8.679100e+00  6.197700e+00]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 20:44:24.516914: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_22270\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1\n",
      "3/3 [==============================] - ETA: 0s - loss: 18.1312 - accuracy: 0.2692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 20:44:33.247770: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_27641\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 11s 3s/step - loss: 20.6115 - accuracy: 0.2788 - val_loss: 48.9774 - val_accuracy: 0.5588\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55882, saving model to ./_best_model_lstm_time_domain.h5\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 7.5463 - accuracy: 0.9691 - val_loss: 77.2696 - val_accuracy: 0.5588\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.55882\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 85.5860 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.55882\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 90.5345 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.55882\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 93.7174 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.55882\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 95.6204 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.55882\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 96.8066 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.55882\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 97.5702 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.55882\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 98.0726 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.55882\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 98.4096 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.55882\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 98.6385 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.55882\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model,history = train_model(X_train, y_train,X_test, y_test, save_to= './', epoch = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9db71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
