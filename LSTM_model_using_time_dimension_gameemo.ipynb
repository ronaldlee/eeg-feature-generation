{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f620c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, GRU, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_df = pd.read_csv('./csv/out_gameemo_time_domain_simple.csv',  sep=',')\n",
    "\n",
    "print('Shape of data: ', init_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5562cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = init_df.copy()\n",
    "print(df.head())\n",
    "\n",
    "#HA_PV = high arousal, positive valence\n",
    "#HA_NV = high arousal, negative valence\n",
    "#LA_NV = low arousal, negative valence\n",
    "#LA_PV = low arousal, positive valance\n",
    "\n",
    "label_map = {1:\"HA_PV\", 2:\"HA_NV\", 3:\"LA_NV\", 4:\"LA_PV\"}\n",
    "\n",
    "df[\"Label\"] = df[\"Label\"].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.head())\n",
    "\n",
    "features = df.iloc[:, :-1]\n",
    "label = df.iloc[:, -1:]\n",
    "\n",
    "print('Shape of data: ', df.shape)\n",
    "print('features.shape: ', features.shape)\n",
    "print('label.shape: ', label.shape)\n",
    "\n",
    "df.head()\n",
    "print(df.columns)\n",
    "\n",
    "y = label.to_numpy()\n",
    "X = features.to_numpy()\n",
    "\n",
    "# #normalize\n",
    "# X = (X - np.min(X))/(np.max(X) - np.min(X))\n",
    "# print(\"X max:\", X.max(),\"; mean:\", X.min())\n",
    "# print(\"X head:\", X[0:10])\n",
    "\n",
    "# 38252 is the max sample size, data collected for one participant. Can choose smaller sample size that can\n",
    "# divide 38252.\n",
    "# 38252 can be divided by 73 or 131, 524\n",
    "sample_size = int(38252/73)  \n",
    "num_of_features = 14\n",
    "\n",
    "train_dataset_percentage = 0.7\n",
    "\n",
    "print(\"sample_size:\",sample_size)\n",
    "print(\"num_of_features:\",num_of_features)\n",
    "\n",
    "total_samples_count = int(X.shape[0]/sample_size)\n",
    "\n",
    "print(\"total_samples_count:\", total_samples_count)\n",
    "\n",
    "\n",
    "train_sample_count = int(total_samples_count * train_dataset_percentage)\n",
    "test_sample_count = total_samples_count - train_sample_count\n",
    "\n",
    "train_size = train_sample_count * sample_size\n",
    "test_size = test_sample_count * sample_size\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"test size:\", test_size)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "X_train = X_train.reshape((train_sample_count,sample_size,num_of_features))\n",
    "X_test = X_test.reshape((test_sample_count,sample_size,num_of_features))\n",
    "\n",
    "print(\"X_train.shape after reshape:\",X_train.shape)\n",
    "print(\"X_test.shape after reshape:\",X_test.shape)\n",
    "\n",
    "#collapse y_train and y_test to the same X sample counts instead\n",
    "\n",
    "y_train_collapsed = np.array([])\n",
    "for i in range(len(y_train)):\n",
    "    if (i % sample_size == 0):\n",
    "        y_train_collapsed = np.append(y_train_collapsed, (y_train[i]))\n",
    "        \n",
    "print(\"y_train_collapsed shape:\",y_train_collapsed.shape)        \n",
    "\n",
    "y_test_collapsed = np.array([])\n",
    "for i in range(len(y_test)):\n",
    "    if (i % sample_size == 0):\n",
    "        y_test_collapsed = np.append(y_test_collapsed, (y_test[i]))\n",
    "        \n",
    "print(\"y_test_collapsed shape:\",y_test_collapsed.shape)    \n",
    "\n",
    "\n",
    "y_train = pd.get_dummies(y_train_collapsed)\n",
    "y_test = pd.get_dummies(y_test_collapsed)\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_loss_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_accuracy_history(history):\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['accuracy'] + 1)))\n",
    "  plt.plot(history['accuracy'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_accuracy'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train,x_test,y_test, save_to, epoch, sample_size, num_of_features):\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))  \n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    \n",
    "    with strategy.scope():\n",
    "\n",
    "#         inputs = tf.keras.Input(shape=(sample_size,num_of_features))\n",
    "#         #ml_model = tf.keras.layers.GRU(256, return_sequences=True)(inputs)\n",
    "#         ml_model = tf.keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
    "#         flat = Flatten()(ml_model)\n",
    "#         outputs = Dense(4, activation='softmax')(flat)\n",
    "#         model = tf.keras.Model(inputs, outputs)\n",
    "        \n",
    "        ######\n",
    "        #sample size:38252/524, accuracy: 1.0000 - val_loss: 5.5889 - val_accuracy: 0.4403\n",
    "        #sample size:38252/73, loss: 1.4733e-04 - accuracy: 1.0000 - val_loss: 4.6835 - val_accuracy: 0.5475\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, return_sequences=True, input_shape=(sample_size,num_of_features)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # try add dense layer, val_accruacy: 0.39        \n",
    "#         model.add(tf.keras.layers.Dense(\n",
    "#             units=1024,\n",
    "#             name='fc_1', \n",
    "#             activation='relu'))\n",
    "\n",
    "#         # add dropout layer\n",
    "#         model.add(tf.keras.layers.Dropout(\n",
    "#             rate=0.5))\n",
    "        \n",
    "        model.add(Dense(4))\n",
    "        model.add(Activation('softmax'))        \n",
    "        \n",
    "        ######\n",
    "#         model = Sequential()\n",
    "#         model.add(LSTM(256, return_sequences=True, input_shape=(sample_size,num_of_features), go_backwards=True))\n",
    "#         model.add(Flatten())\n",
    "#         model.add(Dense(4))\n",
    "#         model.add(Activation('softmax'))\n",
    "\n",
    "        ######\n",
    "        \n",
    "#         model = Sequential()\n",
    "#         model.add(Bidirectional(LSTM(256, return_sequences=True), \n",
    "#                                 input_shape=(sample_size,num_of_features))) #, merge_mode='concat'))\n",
    "#         model.add(Flatten())\n",
    "#         model.add(Dense(4))\n",
    "#         model.add(Activation('softmax')) \n",
    "        \n",
    "\n",
    "        model.summary()\n",
    "        tf.keras.utils.plot_model(model)\n",
    "\n",
    "        opt_adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + '_best_model_lstm_time_domain.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "            \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "            \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                      loss=['categorical_crossentropy'],\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "          \n",
    "    history = model.fit(x_train,y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(x_test,y_test),\n",
    "                        callbacks=[es,mc,lr_schedule], shuffle=False)\n",
    "        \n",
    "    # saved_model = load_model(save_to + '_best_model_lstm_all_cat.h5')\n",
    "        \n",
    "    return model,history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model,history = train_model(X_train, y_train,X_test, y_test, save_to= './', epoch = 40, \n",
    "                            sample_size=sample_size, num_of_features=num_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't plot, big image data to commit to git.\n",
    "# history_data = pd.DataFrame(history.history)\n",
    "# plot_loss_history(history_data)\n",
    "# plot_accuracy_history(history_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3033846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
