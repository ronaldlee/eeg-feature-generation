{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f620c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 12:17:01.197476: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, GRU, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9c4d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (58576, 2703)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_df = pd.read_csv('./csv/sample_size_523_output_gameemo.csv',  sep=',')\n",
    "\n",
    "print('Shape of data: ', init_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b5562cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
      "0    24.879771     3.106697    -1.239877    -0.555734   -17.584463   \n",
      "1    -0.440098    -1.449679     9.304872     1.034350    -0.725376   \n",
      "2     0.829206    -3.484418    -1.650308     0.111657     0.766096   \n",
      "3     1.930660    -1.260540     8.699489     2.875833     7.494725   \n",
      "4     3.259741   -20.054091    -2.742413     2.732820   -23.647910   \n",
      "\n",
      "   lag1_mean_5  lag1_mean_6  lag1_mean_7  lag1_mean_8  lag1_mean_9  ...  \\\n",
      "0   -17.821559   -12.738755   -16.915080     2.374827    -2.715987  ...   \n",
      "1    -2.137903    -0.015417     0.565977    -0.992917     1.060846  ...   \n",
      "2    -4.615465    -1.194480    -0.970423     0.958364     4.277605  ...   \n",
      "3    -3.528349     3.550838     0.847434    -0.780957    -1.208350  ...   \n",
      "4    -7.475699    -4.147064     0.274457     5.449123     6.032328  ...   \n",
      "\n",
      "   freq_282_13  freq_292_13  freq_302_13  freq_313_13  freq_323_13  \\\n",
      "0     0.015183     0.007817     0.016656     0.011823     0.005104   \n",
      "1     0.011079     0.006359     0.009360     0.004060     0.007449   \n",
      "2     0.012525     0.006282     0.009817     0.012705     0.013459   \n",
      "3     0.031533     0.029382     0.049258     0.024216     0.042694   \n",
      "4     0.013622     0.013242     0.013979     0.004138     0.004869   \n",
      "\n",
      "   freq_334_13  freq_344_13  freq_355_13  freq_365_13  Label  \n",
      "0     0.012161     0.007993     0.001592     0.002741    4.0  \n",
      "1     0.001165     0.003843     0.006107     0.004177    4.0  \n",
      "2     0.006619     0.001722     0.004766     0.008656    4.0  \n",
      "3     0.021485     0.026787     0.017933     0.023372    2.0  \n",
      "4     0.003764     0.008181     0.001830     0.002999    4.0  \n",
      "\n",
      "[5 rows x 2703 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = init_df.copy()\n",
    "print(df.head())\n",
    "\n",
    "#HA_PV = high arousal, positive valence\n",
    "#HA_NV = high arousal, negative valence\n",
    "#LA_NV = low arousal, negative valence\n",
    "#LA_PV = low arousal, positive valance\n",
    "\n",
    "label_map = {1:\"HA_PV\", 2:\"HA_NV\", 3:\"LA_NV\", 4:\"LA_PV\"}\n",
    "\n",
    "df[\"Label\"] = df[\"Label\"].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9eb7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
      "0    24.879771     3.106697    -1.239877    -0.555734   -17.584463   \n",
      "1    -0.440098    -1.449679     9.304872     1.034350    -0.725376   \n",
      "2     0.829206    -3.484418    -1.650308     0.111657     0.766096   \n",
      "3     1.930660    -1.260540     8.699489     2.875833     7.494725   \n",
      "4     3.259741   -20.054091    -2.742413     2.732820   -23.647910   \n",
      "\n",
      "   lag1_mean_5  lag1_mean_6  lag1_mean_7  lag1_mean_8  lag1_mean_9  ...  \\\n",
      "0   -17.821559   -12.738755   -16.915080     2.374827    -2.715987  ...   \n",
      "1    -2.137903    -0.015417     0.565977    -0.992917     1.060846  ...   \n",
      "2    -4.615465    -1.194480    -0.970423     0.958364     4.277605  ...   \n",
      "3    -3.528349     3.550838     0.847434    -0.780957    -1.208350  ...   \n",
      "4    -7.475699    -4.147064     0.274457     5.449123     6.032328  ...   \n",
      "\n",
      "   freq_282_13  freq_292_13  freq_302_13  freq_313_13  freq_323_13  \\\n",
      "0     0.015183     0.007817     0.016656     0.011823     0.005104   \n",
      "1     0.011079     0.006359     0.009360     0.004060     0.007449   \n",
      "2     0.012525     0.006282     0.009817     0.012705     0.013459   \n",
      "3     0.031533     0.029382     0.049258     0.024216     0.042694   \n",
      "4     0.013622     0.013242     0.013979     0.004138     0.004869   \n",
      "\n",
      "   freq_334_13  freq_344_13  freq_355_13  freq_365_13  Label  \n",
      "0     0.012161     0.007993     0.001592     0.002741  LA_PV  \n",
      "1     0.001165     0.003843     0.006107     0.004177  LA_PV  \n",
      "2     0.006619     0.001722     0.004766     0.008656  LA_PV  \n",
      "3     0.021485     0.026787     0.017933     0.023372  HA_NV  \n",
      "4     0.003764     0.008181     0.001830     0.002999  LA_PV  \n",
      "\n",
      "[5 rows x 2703 columns]\n",
      "Shape of data:  (58576, 2703)\n",
      "features.shape:  (58576, 2702)\n",
      "label.shape:  (58576, 1)\n",
      "sample_size: 523\n",
      "num_of_features: 2702\n",
      "total_samples_count: 112\n",
      "train size: 40794\n",
      "test size: 17782\n",
      "train.shape: (40794, 2702)\n",
      "test.shape: (17782, 2702)\n",
      "X_train.shape after reshape: (78, 523, 2702)\n",
      "X_test.shape after reshape: (34, 523, 2702)\n",
      "y_train_collapsed shape: (78,)\n",
      "y_test_collapsed shape: (34,)\n",
      "y_train.shape: (78, 4)\n",
      "y_test.shape: (34, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.head())\n",
    "\n",
    "features = df.iloc[:, :-1]\n",
    "label = df.iloc[:, -1:]\n",
    "\n",
    "print('Shape of data: ', df.shape)\n",
    "print('features.shape: ', features.shape)\n",
    "print('label.shape: ', label.shape)\n",
    "\n",
    "# df.head()\n",
    "# print(df.columns)\n",
    "\n",
    "y = label.to_numpy()\n",
    "X = features.to_numpy()\n",
    "\n",
    "############################################################\n",
    "# total number of batches: 8064 \n",
    "# timesteps per batch: 72\n",
    "# features: 8974 (minus one for the label at the end)\n",
    "\n",
    "num_of_features = X.shape[1]\n",
    "\n",
    "# sample_size = 72  \n",
    "# num_of_features = 8974\n",
    "\n",
    "sample_size = 523  \n",
    "\n",
    "train_dataset_percentage = 0.7\n",
    "\n",
    "print(\"sample_size:\",sample_size)\n",
    "print(\"num_of_features:\",num_of_features)\n",
    "\n",
    "total_samples_count = int(X.shape[0]/sample_size)\n",
    "\n",
    "print(\"total_samples_count:\", total_samples_count)\n",
    "\n",
    "\n",
    "train_sample_count = int(total_samples_count * train_dataset_percentage)\n",
    "test_sample_count = total_samples_count - train_sample_count\n",
    "\n",
    "train_size = train_sample_count * sample_size\n",
    "test_size = test_sample_count * sample_size\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"test size:\", test_size)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(\"train.shape:\", X_train.shape)\n",
    "print(\"test.shape:\", X_test.shape)\n",
    "\n",
    "X_train = X_train.reshape((train_sample_count,sample_size,num_of_features))\n",
    "X_test = X_test.reshape((test_sample_count,sample_size,num_of_features))\n",
    "\n",
    "print(\"X_train.shape after reshape:\",X_train.shape)\n",
    "print(\"X_test.shape after reshape:\",X_test.shape)\n",
    "\n",
    "#collapse y_train and y_test to the same X sample counts instead\n",
    "\n",
    "y_train_collapsed = np.array([])\n",
    "for i in range(len(y_train)):\n",
    "    if (i % sample_size == 0):\n",
    "        y_train_collapsed = np.append(y_train_collapsed, (y_train[i]))\n",
    "        \n",
    "print(\"y_train_collapsed shape:\",y_train_collapsed.shape)        \n",
    "\n",
    "y_test_collapsed = np.array([])\n",
    "for i in range(len(y_test)):\n",
    "    if (i % sample_size == 0):\n",
    "        y_test_collapsed = np.append(y_test_collapsed, (y_test[i]))\n",
    "        \n",
    "print(\"y_test_collapsed shape:\",y_test_collapsed.shape)    \n",
    "\n",
    "# one-hot encoding\n",
    "y_train = pd.get_dummies(y_train_collapsed)\n",
    "y_test = pd.get_dummies(y_test_collapsed)\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2fa0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train,x_test,y_test, save_to, epoch, sample_size, num_of_features):\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))  \n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    \n",
    "    with strategy.scope():\n",
    "\n",
    "#         inputs = tf.keras.Input(shape=(sample_size,num_of_features))\n",
    "#         #ml_model = tf.keras.layers.GRU(256, return_sequences=True)(inputs)\n",
    "#         ml_model = tf.keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
    "#         flat = Flatten()(ml_model)\n",
    "#         outputs = Dense(4, activation='softmax')(flat)\n",
    "#         model = tf.keras.Model(inputs, outputs)\n",
    "        \n",
    "        ######\n",
    "        #sample size:38252/524, accuracy: 1.0000 - val_loss: 5.5889 - val_accuracy: 0.4403\n",
    "        #sample size:38252/73, loss: 1.4733e-04 - accuracy: 1.0000 - val_loss: 4.6835 - val_accuracy: 0.5475\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, return_sequences=True, input_shape=(sample_size,num_of_features)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4))\n",
    "        model.add(Activation('softmax'))        \n",
    "        \n",
    "        ######\n",
    "#         model = Sequential()\n",
    "#         model.add(LSTM(256, return_sequences=True, input_shape=(sample_size,num_of_features), go_backwards=True))\n",
    "#         model.add(Flatten())\n",
    "#         model.add(Dense(4))\n",
    "#         model.add(Activation('softmax'))\n",
    "\n",
    "        ######\n",
    "        \n",
    "#         model = Sequential()\n",
    "#         model.add(Bidirectional(LSTM(256, return_sequences=True), \n",
    "#                                 input_shape=(sample_size,num_of_features))) #, merge_mode='concat'))\n",
    "#         model.add(Flatten())\n",
    "#         model.add(Dense(4))\n",
    "#         model.add(Activation('softmax')) \n",
    "        \n",
    "\n",
    "        model.summary()\n",
    "        tf.keras.utils.plot_model(model)\n",
    "\n",
    "        opt_adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + '_best_model_lstm_time_domain.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "            \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "            \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                      loss=['categorical_crossentropy'],\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "          \n",
    "    history = model.fit(x_train,y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(x_test,y_test),\n",
    "                        callbacks=[es,mc,lr_schedule], shuffle=False)\n",
    "        \n",
    "    # saved_model = load_model(save_to + '_best_model_lstm_all_cat.h5')\n",
    "        \n",
    "    return model,history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "913c157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 523, 256)          3030016   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 133888)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 535556    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 3,565,572\n",
      "Trainable params: 3,565,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 12:45:34.325811: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_6166\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3/3 [==============================] - ETA: 0s - loss: 4.2561 - accuracy: 0.2962"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 12:45:40.331348: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_8777\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 8s 2s/step - loss: 4.7401 - accuracy: 0.2959 - val_loss: 11.0415 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.23529, saving model to ./_best_model_lstm_time_domain.h5\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 3s 1s/step - loss: 6.8877 - accuracy: 0.2091 - val_loss: 5.1344 - val_accuracy: 0.2059\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.23529\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.6832 - accuracy: 0.7493 - val_loss: 4.0848 - val_accuracy: 0.2059\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.23529\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.1526 - accuracy: 0.9431 - val_loss: 2.0326 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.23529\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.1505 - val_accuracy: 0.3235\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.23529 to 0.32353, saving model to ./_best_model_lstm_time_domain.h5\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 2.5781 - val_accuracy: 0.3529\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.32353 to 0.35294, saving model to ./_best_model_lstm_time_domain.h5\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.3124 - val_accuracy: 0.3235\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.35294\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 3s 988ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.0812 - val_accuracy: 0.2647\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.35294\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 3s 999ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.9651 - val_accuracy: 0.2647\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.35294\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 3s 985ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.9085 - val_accuracy: 0.2941\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.35294\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 3s 974ms/step - loss: 8.8346e-04 - accuracy: 1.0000 - val_loss: 1.8627 - val_accuracy: 0.2353\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.35294\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 3s 988ms/step - loss: 7.1394e-04 - accuracy: 1.0000 - val_loss: 1.8313 - val_accuracy: 0.2059\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.35294\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 3s 992ms/step - loss: 5.8312e-04 - accuracy: 1.0000 - val_loss: 1.7981 - val_accuracy: 0.1471\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.35294\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 3s 994ms/step - loss: 4.8102e-04 - accuracy: 1.0000 - val_loss: 1.7647 - val_accuracy: 0.1471\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.35294\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 3s 996ms/step - loss: 4.0288e-04 - accuracy: 1.0000 - val_loss: 1.7387 - val_accuracy: 0.1471\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.35294\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 3s 968ms/step - loss: 3.4235e-04 - accuracy: 1.0000 - val_loss: 1.7213 - val_accuracy: 0.1471\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.35294\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 3s 981ms/step - loss: 2.9462e-04 - accuracy: 1.0000 - val_loss: 1.7095 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.35294\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 3s 981ms/step - loss: 2.5929e-04 - accuracy: 1.0000 - val_loss: 1.7003 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.35294\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 3s 976ms/step - loss: 2.3372e-04 - accuracy: 1.0000 - val_loss: 1.6953 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.35294\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 3s 998ms/step - loss: 2.1469e-04 - accuracy: 1.0000 - val_loss: 1.6940 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.35294\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 3s 963ms/step - loss: 2.0041e-04 - accuracy: 1.0000 - val_loss: 1.6949 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.35294\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 3s 977ms/step - loss: 1.8960e-04 - accuracy: 1.0000 - val_loss: 1.6979 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.35294\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.8128e-04 - accuracy: 1.0000 - val_loss: 1.7021 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.35294\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 3s 972ms/step - loss: 1.7479e-04 - accuracy: 1.0000 - val_loss: 1.7065 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.35294\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 3s 976ms/step - loss: 1.6977e-04 - accuracy: 1.0000 - val_loss: 1.7106 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.35294\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 3s 953ms/step - loss: 1.6587e-04 - accuracy: 1.0000 - val_loss: 1.7143 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.35294\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 3s 976ms/step - loss: 1.6284e-04 - accuracy: 1.0000 - val_loss: 1.7176 - val_accuracy: 0.1765\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.35294\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 3s 975ms/step - loss: 1.6046e-04 - accuracy: 1.0000 - val_loss: 1.7204 - val_accuracy: 0.2059\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.35294\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 3s 983ms/step - loss: 1.5851e-04 - accuracy: 1.0000 - val_loss: 1.7228 - val_accuracy: 0.2059\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.35294\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 3s 996ms/step - loss: 1.5693e-04 - accuracy: 1.0000 - val_loss: 1.7249 - val_accuracy: 0.2059\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.35294\n",
      "Epoch 00030: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model,history = train_model(X_train, y_train,X_test, y_test, save_to= './', epoch = 40, \n",
    "                            sample_size=sample_size, num_of_features=num_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_accuracy_history(history):\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['accuracy'] + 1)))\n",
    "  plt.plot(history['accuracy'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_accuracy'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_data = pd.DataFrame(history.history)\n",
    "plot_loss_history(history_data)\n",
    "plot_accuracy_history(history_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
