{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aceecaa1",
   "metadata": {},
   "source": [
    "<h1>CNN model on GAMEEMO raw dataset, 7 secs grouping</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f620c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #only show errors (hide INFO and WARNING)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "\n",
    "random.seed(1234)   \n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db91375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fd893",
   "metadata": {},
   "source": [
    "<h3>Helper functions to plot loss/accuracy, and build/train/test models.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d19f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_accuracy_history(history):\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['accuracy'] + 1)))\n",
    "  plt.plot(history['accuracy'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_accuracy'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88da026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sample_size, num_of_features, dense_layers_unit_array=[], learningRate=0.001,\n",
    "                activation=\"relu\", isBatchNormalized=False, dropOutRate=0,\n",
    "                startWithBatchNormalized=False,optimizer=\"Adam\",\n",
    "                conv_layers_filters_array=[],conv_kernel_size_array=[],conv_strides_array=[]\n",
    "               ):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    \n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    print('Number of GPU/CPU: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    print(\"num_of_dense_layers:\",len(dense_layers_unit_array))\n",
    "    \n",
    "    for i, dense_layer_unit in enumerate(dense_layers_unit_array):       \n",
    "        print(\"dense_layer[\"+str(i)+\"]; unit:\"+str(dense_layer_unit))      \n",
    "        \n",
    "        \n",
    "    print(\"num_of_conv_layers:\",len(conv_layers_filters_array))    \n",
    "    for i, conv_layer_filters in enumerate(conv_layers_filters_array):       \n",
    "        print(\"conv_layer_filters[\"+str(i)+\"]; unit:\"+str(conv_layer_filters))  \n",
    "        print(\"conv_kernel[\"+str(i)+\"]; unit:\"+str(conv_kernel_size_array[i]))\n",
    "        print(\"conv_strides[\"+str(i)+\"]; unit:\"+str(conv_strides_array[i]))\n",
    "       \n",
    "    print(\"learningRate:\",learningRate)\n",
    "    print(\"isBatchNormalized:\",isBatchNormalized,\"; dropOutRate:\",dropOutRate)\n",
    "    \n",
    "    startWithBatchNormalized = startWithBatchNormalized or optimizer == \"SGD\"\n",
    "    print(\"startWithBatchNormalized:\",startWithBatchNormalized)\n",
    "    print(\"optimizer:\",optimizer,\"; activation:\",activation)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(1234)     \n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    \n",
    "    with strategy.scope():   \n",
    "        model = Sequential() \n",
    "        \n",
    "        if startWithBatchNormalized:\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # Add Conv2D layers\n",
    "        for i, conv_layer_filters in enumerate(conv_layers_filters_array):    \n",
    "            model.add(tf.keras.layers.Conv2D(\n",
    "            filters=conv_layer_filters,                \n",
    "            kernel_size=conv_kernel_size_array[i],\n",
    "            strides=conv_strides_array[i],\n",
    "            padding='same',\n",
    "            data_format='channels_last',\n",
    "            name='conv_'+str(i),\n",
    "            activation='relu'))\n",
    "        \n",
    "            model.add(tf.keras.layers.MaxPool2D(\n",
    "                pool_size=2,\n",
    "                name='pool_'+str(i))) \n",
    "\n",
    "        if len(conv_layers_filters_array) > 0:    \n",
    "            model.add(Flatten()) \n",
    "        \n",
    "        # Add Dense layers\n",
    "        for i, dense_layer_unit in enumerate(dense_layers_unit_array):\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                units=dense_layer_unit,\n",
    "                name='fc_'+str(i), \n",
    "                activation=activation))\n",
    "            if isBatchNormalized:\n",
    "                model.add(BatchNormalization())\n",
    "            if dropOutRate > 0:\n",
    "                model.add(Dropout(dropOutRate))\n",
    "\n",
    "        model.add(Dense(4))\n",
    "        model.add(Activation('softmax'))   \n",
    "\n",
    "        model.build(input_shape=(None, sample_size, num_of_features, 1))\n",
    "\n",
    "        if optimizer==\"Adam\":\n",
    "            opt = keras.optimizers.Adam(learning_rate=learningRate)\n",
    "        elif optimizer==\"SGD\":\n",
    "            opt = keras.optimizers.SGD(learning_rate=learningRate)\n",
    "        else:\n",
    "            opt = keras.optimizers.Adam(learning_rate=learningRate)    \n",
    "        \n",
    "\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=['categorical_crossentropy'],\n",
    "                      metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(sample_size, num_of_features,\n",
    "                         train_dataset, validate_dataset, x_train,\n",
    "                         save_to, epoch = 2,\n",
    "                         dense_layers_unit_array=[],\n",
    "                         patience=10, epoch_denominator=10.,\n",
    "                         isConstantLearningRate=False, learningRate=0.001,\n",
    "                         activation=\"relu\", isBatchNormalized=False,dropOutRate=0,\n",
    "                         startWithBatchNormalized=False,optimizer=\"Adam\",\n",
    "                         conv_layers_filters_array=[],conv_kernel_size_array=[],conv_strides_array=[]\n",
    "                        ):\n",
    "    \n",
    "    print(\"epoch:\",epoch, \"; epoch_denominator:\",epoch_denominator) \n",
    "    print(\"patience:\",patience,\"; isConstantLearningRate:\", isConstantLearningRate) \n",
    "    \n",
    "    model = build_model(sample_size, num_of_features,\n",
    "                        dense_layers_unit_array=dense_layers_unit_array, learningRate=learningRate,\n",
    "                        activation=activation, isBatchNormalized=isBatchNormalized, dropOutRate=dropOutRate,\n",
    "                        startWithBatchNormalized=startWithBatchNormalized,optimizer=optimizer,\n",
    "                        conv_layers_filters_array=conv_layers_filters_array,\n",
    "                        conv_kernel_size_array=conv_kernel_size_array,\n",
    "                        conv_strides_array=conv_strides_array\n",
    "                       )\n",
    "\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model)  \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "    if isConstantLearningRate:  \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learningRate)        \n",
    "    else:  \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learningRate * np.exp(-epoch / epoch_denominator))\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=validate_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[es,lr_schedule])\n",
    "\n",
    "    history_data = pd.DataFrame(history.history)\n",
    "    plot_loss_history(history_data)\n",
    "    plot_accuracy_history(history_data)\n",
    "\n",
    "    # test model\n",
    "    test_results = model.evaluate(test_dataset)\n",
    "    print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))\n",
    "\n",
    "    # show classification report\n",
    "    y_predict = np.array(model.predict(test_dataset))\n",
    "    y_predict = to_categorical(np.argmax(y_predict, axis=1), 4)\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "    print(\"Confusion matrix\")\n",
    "    print(confusion_matrix(y_test.values.argmax(axis=1), np.argmax(y_predict, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HA_PV = high arousal, positive valence\n",
    "#HA_NV = high arousal, negative valence\n",
    "#LA_NV = low arousal, negative valence\n",
    "#LA_PV = low arousal, positive valance\n",
    "label_map = {1:\"HA_PV\", 2:\"HA_NV\", 3:\"LA_NV\", 4:\"LA_PV\"}\n",
    "\n",
    "path=\"dataset_gameemo/original_data_with_timestamps\"\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "X_7_sec = []\n",
    "y = []\n",
    "\n",
    "num_of_features = 14\n",
    "num_of_records_per_experiment=37632\n",
    "num_of_samples = 42\n",
    "sample_size=int(num_of_records_per_experiment/num_of_samples)\n",
    "\n",
    "for file in dirs:\n",
    "    df = pd.read_csv('dataset_gameemo/original_data_with_timestamps/' + file)\n",
    "    df.drop(columns='timestamps', inplace=True)\n",
    "    list_of_arrays = np.array_split(df[:num_of_records_per_experiment],num_of_samples)\n",
    "    for array in list_of_arrays:\n",
    "        X_7_sec.append(array)\n",
    "        category = str(file)[4]\n",
    "        y.append(label_map[int(category)])\n",
    "X_7_sec = np.array(X_7_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_7_sec, y, train_size=0.6, random_state=0)\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, train_size=0.5, random_state=0)\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "\n",
    "X_train = np.array(X_train).reshape((X_train.shape[0],X_train.shape[1],X_train.shape[2],1))\n",
    "X_validate = np.array(X_validate).reshape((X_validate.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "X_test = np.array(X_test).reshape((X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_validate = pd.get_dummies(y_validate)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_validate.shape:\", y_validate.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n",
    "print(\"y_train:\")\n",
    "print(y_train[:5])\n",
    "\n",
    "#######\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "validate_dataset = tf.data.Dataset.from_tensor_slices((X_validate, y_validate))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "train_dataset.with_options(options)\n",
    "validate_dataset.with_options(options)\n",
    "test_dataset.with_options(options)\n",
    "\n",
    "batch_size = 30\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "validate_dataset = validate_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a03033",
   "metadata": {},
   "source": [
    "<h3>Manual runs to get a feel of the hyperparameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070713b",
   "metadata": {},
   "source": [
    "Try single layer different sizes.. they all have massive loss and below 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12854c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50],\n",
    "                     conv_kernel_size_array=[(3,3)],\n",
    "                     conv_strides_array=[(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[100],\n",
    "                     conv_kernel_size_array=[(3,3)],\n",
    "                     conv_strides_array=[(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[25],\n",
    "                     conv_kernel_size_array=[(3,3)],\n",
    "                     conv_strides_array=[(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccc27a",
   "metadata": {},
   "source": [
    "Try 2 layers. Need to lower learning rate to 0.001 to improve accruacy, but is still so low at 37%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb22663",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50],\n",
    "                     conv_kernel_size_array=[(3,3),(3,3)],\n",
    "                     conv_strides_array=[(1,1),(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ab5d2",
   "metadata": {},
   "source": [
    "Try lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.01,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50],\n",
    "                     conv_kernel_size_array=[(3,3),(3,3)],\n",
    "                     conv_strides_array=[(1,1),(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa400c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.001,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50],\n",
    "                     conv_kernel_size_array=[(3,3),(3,3)],\n",
    "                     conv_strides_array=[(1,1),(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8d9d1",
   "metadata": {},
   "source": [
    "Try 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.001,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50,50],\n",
    "                     conv_kernel_size_array=[(3,3),(3,3),(3,3)],\n",
    "                     conv_strides_array=[(1,1),(1,1),(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db23d8e",
   "metadata": {},
   "source": [
    "Try 1 layer, different kernel size. </br>\n",
    "Kernal (14,1) is a kernel same size as the number of features, 1 sample at a time. </br>\n",
    "Test curracy greatly improve to 71.63%. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ffbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.001,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50],\n",
    "                     conv_kernel_size_array=[(14,1)],\n",
    "                     conv_strides_array=[(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2c1f0",
   "metadata": {},
   "source": [
    "Try kernel (14,2) to scan 2 rows at a time, same stride. </br>\n",
    "Accruacy increase a bit more to 74.18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.001,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50],\n",
    "                     conv_kernel_size_array=[(14,2)],\n",
    "                     conv_strides_array=[(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9642da4",
   "metadata": {},
   "source": [
    "Try (14,3) kernel, accruacy starts dropping.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.001,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50],\n",
    "                     conv_kernel_size_array=[(14,3)],\n",
    "                     conv_strides_array=[(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8676eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(sample_size, num_of_features, \n",
    "                     train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.001,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50],\n",
    "                     conv_kernel_size_array=[(1,14)],\n",
    "                     conv_strides_array=[(1,1)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5d275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
