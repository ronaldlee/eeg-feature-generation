{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017adf80",
   "metadata": {},
   "source": [
    "<h1>LSTM + MLPNN model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f620c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #only show errors (hide INFO and WARNING)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "\n",
    "random.seed(1234)   \n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_accuracy_history(history):\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['accuracy'] + 1)))\n",
    "  plt.plot(history['accuracy'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_accuracy'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f745f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(number_of_features, dense_layers_unit_array=[], learningRate=0.001,\n",
    "                activation=\"relu\", isBatchNormalized=False, dropOutRate=0,\n",
    "                startWithBatchNormalized=False,optimizer=\"Adam\",\n",
    "                lstm_layers_unit_array=[]\n",
    "               ):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    \n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    print('Number of GPU/CPU: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    print(\"num_of_dense_layers:\",len(dense_layers_unit_array))\n",
    "    \n",
    "    for i, dense_layer_unit in enumerate(dense_layers_unit_array):       \n",
    "        print(\"dense_layer[\"+str(i)+\"]; unit:\"+str(dense_layer_unit))      \n",
    "        \n",
    "        \n",
    "    print(\"num_of_lstm_layers:\",len(lstm_layers_unit_array))    \n",
    "    for i, lstm_layer_unit in enumerate(lstm_layers_unit_array):       \n",
    "        print(\"lstm_layer_unit[\"+str(i)+\"]; unit:\"+str(lstm_layer_unit))\n",
    "       \n",
    "    print(\"learningRate:\",learningRate)\n",
    "    print(\"isBatchNormalized:\",isBatchNormalized,\"; dropOutRate:\",dropOutRate)\n",
    "    \n",
    "    startWithBatchNormalized = startWithBatchNormalized or optimizer == \"SGD\"\n",
    "    print(\"startWithBatchNormalized:\",startWithBatchNormalized)\n",
    "    print(\"optimizer:\",optimizer,\"; activation:\",activation)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(1234)     \n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    \n",
    "    with strategy.scope():   \n",
    "        model = Sequential() \n",
    "        \n",
    "        if startWithBatchNormalized:\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # Add Dense layers\n",
    "        for i, lstm_layer_unit in enumerate(lstm_layers_unit_array):    \n",
    "            model.add(LSTM(lstm_layer_unit, return_sequences=True))\n",
    "\n",
    "        if len(lstm_layers_unit_array) > 0:    \n",
    "            model.add(Flatten()) \n",
    "        \n",
    "        # Add Dense layers\n",
    "        for i, dense_layer_unit in enumerate(dense_layers_unit_array):\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                units=dense_layer_unit,\n",
    "                name='fc_'+str(i), \n",
    "                activation=activation))\n",
    "            if isBatchNormalized:\n",
    "                model.add(BatchNormalization())\n",
    "            if dropOutRate > 0:\n",
    "                model.add(Dropout(dropOutRate))\n",
    "\n",
    "        model.add(Dense(4))\n",
    "        model.add(Activation('softmax'))   \n",
    "\n",
    "        model.build(input_shape=(None, number_of_features, 1))\n",
    "\n",
    "        if optimizer==\"Adam\":\n",
    "            opt = keras.optimizers.Adam(learning_rate=learningRate)\n",
    "        elif optimizer==\"SGD\":\n",
    "            opt = keras.optimizers.SGD(learning_rate=learningRate)\n",
    "        else:\n",
    "            opt = keras.optimizers.Adam(learning_rate=learningRate)    \n",
    "        \n",
    "\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=['categorical_crossentropy'],\n",
    "                      metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(train_dataset, validate_dataset, x_train,\n",
    "                         save_to, epoch = 2,\n",
    "                         dense_layers_unit_array=[],\n",
    "                         patience=10, epoch_denominator=10.,\n",
    "                         isConstantLearningRate=False, learningRate=0.001,\n",
    "                         activation=\"relu\", isBatchNormalized=False,dropOutRate=0,\n",
    "                         startWithBatchNormalized=False,optimizer=\"Adam\",\n",
    "                         lstm_layers_unit_array=[]\n",
    "                        ):\n",
    "    \n",
    "    print(\"epoch:\",epoch, \"; epoch_denominator:\",epoch_denominator) \n",
    "    print(\"patience:\",patience,\"; isConstantLearningRate:\", isConstantLearningRate) \n",
    "    \n",
    "    model = build_model(x_train.shape[1],\n",
    "                        dense_layers_unit_array=dense_layers_unit_array, learningRate=learningRate,\n",
    "                        activation=activation, isBatchNormalized=isBatchNormalized, dropOutRate=dropOutRate,\n",
    "                        startWithBatchNormalized=startWithBatchNormalized,optimizer=optimizer,\n",
    "                        lstm_layers_unit_array=lstm_layers_unit_array\n",
    "                       )\n",
    "\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model)  \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "    if isConstantLearningRate:  \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learningRate)        \n",
    "    else:  \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learningRate * np.exp(-epoch / epoch_denominator))\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=validate_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[es,lr_schedule])\n",
    "\n",
    "    history_data = pd.DataFrame(history.history)\n",
    "    plot_loss_history(history_data)\n",
    "    plot_accuracy_history(history_data)\n",
    "\n",
    "    # test model\n",
    "    test_results = model.evaluate(test_dataset)\n",
    "    print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))\n",
    "\n",
    "    # show classification report\n",
    "    y_predict = np.array(model.predict(test_dataset))\n",
    "    y_predict = to_categorical(np.argmax(y_predict, axis=1), 4)\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "    print(\"Confusion matrix\")\n",
    "    print(confusion_matrix(y_test.values.argmax(axis=1), np.argmax(y_predict, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('csv/out_gameemo.csv',  sep=',')\n",
    "\n",
    "print('Shape of data: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "init_df = dataset.copy()\n",
    "\n",
    "#HA_PV = high arousal, positive valence\n",
    "#HA_NV = high arousal, negative valence\n",
    "#LA_NV = low arousal, negative valence\n",
    "#LA_PV = low arousal, positive valance\n",
    "label_map = {1:\"HA_PV\", 2:\"HA_NV\", 3:\"LA_NV\", 4:\"LA_PV\"}\n",
    "\n",
    "init_df[\"Label\"] = init_df[\"Label\"].map(label_map)\n",
    "\n",
    "\n",
    "features = init_df.iloc[:, :-1]\n",
    "label = init_df.iloc[:, -1:]\n",
    "\n",
    "print('Shape of data: ', init_df.shape)\n",
    "print('features.shape: ', features.shape)\n",
    "print('label.shape: ', label.shape)\n",
    "\n",
    "#######\n",
    "\n",
    "y = label\n",
    "X = features\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, train_size=0.6, random_state=48)\n",
    "\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, train_size=0.5, random_state=48)\n",
    "\n",
    "X_train = np.array(X_train).reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "X_validate = np.array(X_validate).reshape((X_validate.shape[0],X_test.shape[1],1))\n",
    "X_test = np.array(X_test).reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_validate = pd.get_dummies(y_validate)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "print(\"y_train:\")\n",
    "print(y_train[:5])\n",
    "\n",
    "#######\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "validate_dataset = tf.data.Dataset.from_tensor_slices((X_validate, y_validate))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "train_dataset.with_options(options)\n",
    "validate_dataset.with_options(options)\n",
    "test_dataset.with_options(options)\n",
    "\n",
    "        \n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "validate_dataset = validate_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5666c6",
   "metadata": {},
   "source": [
    "<h3>Manual runs to get a feel of the hyperparameters</h3>\n",
    "Will use the best LSTM model with 78.23% accuracy and build on that with Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_and_test_model(train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[10], learningRate=0.001,\n",
    "                     startWithBatchNormalized=True, optimizer=\"SGD\",dropOutRate=0.2,\n",
    "                     activation=\"relu\",\n",
    "                     lstm_layers_unit_array=[15]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70681e6c",
   "metadata": {},
   "source": [
    "<h3>Perform hyperparam tuning using the LSTM + MLPNN model</h3>\n",
    "Using the hyperparams from the best LSTM model, only hyper tune the Dense layers params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = X_train.shape[1]\n",
    "\n",
    "def model_tuner(hp):\n",
    "    print(\"number_of_features:\", number_of_features)\n",
    "    \n",
    "    dense_layers_unit_array_size = hp.Int('dense_layers_unit_array_size', min_value=1, max_value=3, step=1)\n",
    "    dense_layers_unit_array = []\n",
    "    \n",
    "    for i in range(dense_layers_unit_array_size):\n",
    "        dense_layers_unit_array.append(hp.Int('dl_units_'+str(i), \n",
    "                                              min_value=64, max_value=640, step=64))\n",
    "    \n",
    "    lstm_layers_unit_array=[15]\n",
    "    \n",
    "    model = build_model(number_of_features, \n",
    "                dense_layers_unit_array=dense_layers_unit_array, \n",
    "                learningRate=hp.Choice('learningRate', values=[0.001, 0.01, 0.05, 0.1]),\n",
    "                activation=hp.Choice('activation', values=[\"tanh\", \"relu\"]), \n",
    "                isBatchNormalized=True, \n",
    "                dropOutRate=0.2,\n",
    "                startWithBatchNormalized= False,      \n",
    "                optimizer=\"Adam\",\n",
    "                lstm_layers_unit_array=lstm_layers_unit_array\n",
    "                       )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ead56",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "epochs = 40\n",
    "tuner = kt.Hyperband(model_tuner,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=epochs,\n",
    "                     factor=3,\n",
    "                     directory='.',\n",
    "                     seed=1234,\n",
    "                     project_name='eeg')\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "\n",
    "tuner.search(train_dataset, \n",
    "             epochs=epochs, \n",
    "             validation_data=validate_dataset,\n",
    "             shuffle=True,\n",
    "             callbacks=[stop_early])\n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"#### Hypertuning results:\")\n",
    "\n",
    "print(\"learningRate:\",best_hps.get('learningRate'))\n",
    "print(\"isBatchNormalized:\",best_hps.get('isBatchNormalized'))\n",
    "print(\"dropOutRate:\",best_hps.get('dropOutRate'))\n",
    "\n",
    "print(\"dense_layers_unit_array_size:\",best_hps.get('dense_layers_unit_array_size'))\n",
    "try:\n",
    "    print(\"dl_units_0:\",best_hps.get('dl_units_0'))\n",
    "except:\n",
    "    print(\"no dense layer 0\")\n",
    "try:\n",
    "    print(\"dl_units_1:\",best_hps.get('dl_units_1'))\n",
    "except:\n",
    "    print(\"no dense layer 1\")\n",
    "try:\n",
    "    print(\"dl_units_2:\",best_hps.get('dl_units_2'))\n",
    "except:\n",
    "    print(\"no dense layer 2\")    \n",
    "  \n",
    "\n",
    "print(\"dropOutRate:\",best_hps.get('dropOutRate'))\n",
    "\n",
    "print(\"#########################################\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_dataset, \n",
    "             epochs=epochs, \n",
    "             validation_data=validate_dataset,\n",
    "             shuffle=True,\n",
    "             callbacks=[stop_early])\n",
    "\n",
    "history_data = pd.DataFrame(history.history)\n",
    "plot_loss_history(history_data)\n",
    "plot_accuracy_history(history_data)\n",
    "\n",
    "# test model\n",
    "test_results = model.evaluate(test_dataset)\n",
    "print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))\n",
    "\n",
    "# show classification report\n",
    "y_predict = np.array(model.predict(test_dataset))\n",
    "y_predict = to_categorical(np.argmax(y_predict, axis=1), 4)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fcae2b",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "\n",
    "Hyperparam tuning on CNN: 78.23% accuracy </br>\n",
    "num_of_dense_layers: 0 </br>\n",
    "num_of_lstm_layers: 1 </br>\n",
    "lstm_layer_unit[0]; unit:15 </br>\n",
    "learningRate: 0.001 </br>\n",
    "isBatchNormalized: True ; dropOutRate: 0.2 </br>\n",
    "startWithBatchNormalized: False </br>\n",
    "optimizer: Adam ; activation: tanh </br>\n",
    "</br>\n",
    "\n",
    "Manual tuning on CNN: 76.57% accuracy </br>\n",
    "num_of_dense_layers: 0 </br>\n",
    "num_of_lstm_layers: 1 </br>\n",
    "lstm_layer_unit[0]; unit:256 </br>\n",
    "learningRate: 0.001 </br>\n",
    "isBatchNormalized: False ; dropOutRate: 0.5 </br>\n",
    "startWithBatchNormalized: False </br>\n",
    "optimizer: Adam ; activation: relu </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bd6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
