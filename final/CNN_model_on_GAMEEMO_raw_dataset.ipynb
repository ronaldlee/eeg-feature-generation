{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aceecaa1",
   "metadata": {},
   "source": [
    "<h1>CNN model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f620c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #only show errors (hide INFO and WARNING)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "\n",
    "random.seed(1234)   \n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fd893",
   "metadata": {},
   "source": [
    "<h3>Helper functions to plot loss/accuracy, and build/train/test models.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d19f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_accuracy_history(history):\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['accuracy'] + 1)))\n",
    "  plt.plot(history['accuracy'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_accuracy'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88da026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(number_of_features, dense_layers_unit_array=[], learningRate=0.001,\n",
    "                activation=\"relu\", isBatchNormalized=False, dropOutRate=0,\n",
    "                startWithBatchNormalized=False,optimizer=\"Adam\",\n",
    "                conv_layers_filters_array=[],conv_kernel_size_array=[],conv_strides_array=[]\n",
    "               ):\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    \n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    print('Number of GPU/CPU: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    print(\"num_of_dense_layers:\",len(dense_layers_unit_array))\n",
    "    \n",
    "    for i, dense_layer_unit in enumerate(dense_layers_unit_array):       \n",
    "        print(\"dense_layer[\"+str(i)+\"]; unit:\"+str(dense_layer_unit))      \n",
    "        \n",
    "        \n",
    "    print(\"num_of_conv_layers:\",len(conv_layers_filters_array))    \n",
    "    for i, conv_layer_filters in enumerate(conv_layers_filters_array):       \n",
    "        print(\"conv_layer_filters[\"+str(i)+\"]; unit:\"+str(conv_layer_filters))  \n",
    "        print(\"conv_kernel[\"+str(i)+\"]; unit:\"+str(conv_kernel_size_array[i]))\n",
    "        print(\"conv_strides[\"+str(i)+\"]; unit:\"+str(conv_strides_array[i]))\n",
    "       \n",
    "    print(\"learningRate:\",learningRate)\n",
    "    print(\"isBatchNormalized:\",isBatchNormalized,\"; dropOutRate:\",dropOutRate)\n",
    "    \n",
    "    startWithBatchNormalized = startWithBatchNormalized or optimizer == \"SGD\"\n",
    "    print(\"startWithBatchNormalized:\",startWithBatchNormalized)\n",
    "    print(\"optimizer:\",optimizer,\"; activation:\",activation)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(1234)     \n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    \n",
    "    with strategy.scope():   \n",
    "        model = Sequential() \n",
    "        \n",
    "        if startWithBatchNormalized:\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # Add Dense layers\n",
    "        for i, conv_layer_filters in enumerate(conv_layers_filters_array):    \n",
    "            model.add(tf.keras.layers.Conv1D(\n",
    "            filters=conv_layer_filters,                \n",
    "            kernel_size=conv_kernel_size_array[i],\n",
    "            strides=conv_strides_array[i],\n",
    "            padding='same',\n",
    "            data_format='channels_last',\n",
    "            name='conv_'+str(i),\n",
    "            activation='relu'))\n",
    "        \n",
    "            model.add(tf.keras.layers.MaxPool1D(\n",
    "                pool_size=2,\n",
    "                name='pool_'+str(i))) \n",
    "\n",
    "        if len(conv_layers_filters_array) > 0:    \n",
    "            model.add(Flatten()) \n",
    "        \n",
    "        # Add Dense layers\n",
    "        for i, dense_layer_unit in enumerate(dense_layers_unit_array):\n",
    "            model.add(tf.keras.layers.Dense(\n",
    "                units=dense_layer_unit,\n",
    "                name='fc_'+str(i), \n",
    "                activation=activation))\n",
    "            if isBatchNormalized:\n",
    "                model.add(BatchNormalization())\n",
    "            if dropOutRate > 0:\n",
    "                model.add(Dropout(dropOutRate))\n",
    "\n",
    "        model.add(Dense(4))\n",
    "        model.add(Activation('softmax'))   \n",
    "\n",
    "        model.build(input_shape=(None, number_of_features, 1))\n",
    "\n",
    "        if optimizer==\"Adam\":\n",
    "            opt = keras.optimizers.Adam(learning_rate=learningRate)\n",
    "        elif optimizer==\"SGD\":\n",
    "            opt = keras.optimizers.SGD(learning_rate=learningRate)\n",
    "        else:\n",
    "            opt = keras.optimizers.Adam(learning_rate=learningRate)    \n",
    "        \n",
    "\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=['categorical_crossentropy'],\n",
    "                      metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(train_dataset, validate_dataset, x_train,\n",
    "                         save_to, epoch = 2,\n",
    "                         dense_layers_unit_array=[],\n",
    "                         patience=10, epoch_denominator=10.,\n",
    "                         isConstantLearningRate=False, learningRate=0.001,\n",
    "                         activation=\"relu\", isBatchNormalized=False,dropOutRate=0,\n",
    "                         startWithBatchNormalized=False,optimizer=\"Adam\",\n",
    "                         conv_layers_filters_array=[],conv_kernel_size_array=[],conv_strides_array=[]\n",
    "                        ):\n",
    "    \n",
    "    print(\"epoch:\",epoch, \"; epoch_denominator:\",epoch_denominator) \n",
    "    print(\"patience:\",patience,\"; isConstantLearningRate:\", isConstantLearningRate) \n",
    "    \n",
    "    model = build_model(x_train.shape[1],\n",
    "                        dense_layers_unit_array=dense_layers_unit_array, learningRate=learningRate,\n",
    "                        activation=activation, isBatchNormalized=isBatchNormalized, dropOutRate=dropOutRate,\n",
    "                        startWithBatchNormalized=startWithBatchNormalized,optimizer=optimizer,\n",
    "                        conv_layers_filters_array=conv_layers_filters_array,\n",
    "                        conv_kernel_size_array=conv_kernel_size_array,\n",
    "                        conv_strides_array=conv_strides_array\n",
    "                       )\n",
    "\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model)  \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "\n",
    "    if isConstantLearningRate:  \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learningRate)        \n",
    "    else:  \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learningRate * np.exp(-epoch / epoch_denominator))\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=validate_dataset,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[es,lr_schedule])\n",
    "\n",
    "    history_data = pd.DataFrame(history.history)\n",
    "    plot_loss_history(history_data)\n",
    "    plot_accuracy_history(history_data)\n",
    "\n",
    "    # test model\n",
    "    test_results = model.evaluate(test_dataset)\n",
    "    print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))\n",
    "\n",
    "    # show classification report\n",
    "    y_predict = np.array(model.predict(test_dataset))\n",
    "    y_predict = to_categorical(np.argmax(y_predict, axis=1), 4)\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "    print(\"Confusion matrix\")\n",
    "    print(confusion_matrix(y_test.values.argmax(axis=1), np.argmax(y_predict, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('csv/out_gameemo_time_domain_simple.csv',  sep=',')\n",
    "\n",
    "print('Shape of data: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "init_df = dataset.copy()\n",
    "\n",
    "#HA_PV = high arousal, positive valence\n",
    "#HA_NV = high arousal, negative valence\n",
    "#LA_NV = low arousal, negative valence\n",
    "#LA_PV = low arousal, positive valance\n",
    "label_map = {1:\"HA_PV\", 2:\"HA_NV\", 3:\"LA_NV\", 4:\"LA_PV\"}\n",
    "\n",
    "init_df[\"Label\"] = init_df[\"Label\"].map(label_map)\n",
    "\n",
    "\n",
    "features = init_df.iloc[:, :-1]\n",
    "label = init_df.iloc[:, -1:]\n",
    "\n",
    "print('Shape of data: ', init_df.shape)\n",
    "print('features.shape: ', features.shape)\n",
    "print('label.shape: ', label.shape)\n",
    "\n",
    "#######\n",
    "\n",
    "y = label.to_numpy()\n",
    "X = features.to_numpy()\n",
    "\n",
    "# 38252 is the max sample size, data collected for one participant. Can choose smaller sample size that can\n",
    "# divide 38252.\n",
    "# 38252 can be divided by 73 or 131, 524\n",
    "sample_size = int(38252/73)  \n",
    "num_of_features = 14\n",
    "\n",
    "train_dataset_percentage = 0.6\n",
    "\n",
    "print(\"sample_size:\",sample_size)\n",
    "print(\"num_of_features:\",num_of_features)\n",
    "\n",
    "total_samples_count = int(X.shape[0]/sample_size)\n",
    "\n",
    "print(\"total_samples_count:\", total_samples_count)\n",
    "\n",
    "\n",
    "train_sample_count = int(total_samples_count * train_dataset_percentage)\n",
    "validate_sample_count = (total_samples_count - train_sample_count) * 0.5 # half of 40% = 20%\n",
    "test_sample_count = total_samples_count - train_sample_count - validate_sample_count\n",
    "\n",
    "train_size = train_sample_count * sample_size\n",
    "validate_size = validate_sample_count * sample_size\n",
    "test_size = test_sample_count * sample_size\n",
    "\n",
    "print(\"train size:\", train_size)\n",
    "print(\"validate size:\", validate_size)\n",
    "print(\"test size:\", test_size)\n",
    "\n",
    "X_train, X_validate, X_test = X[:train_size], X[train_size:train_size+validate_size], X[validate_size:]\n",
    "y_train, y_validate, y_test = y[:train_size], y[train_size:train_size+validate_size], y[validate_size:]\n",
    "\n",
    "X_train = X_train.reshape((train_sample_count,sample_size,num_of_features))\n",
    "X_validate = X_validate.reshape((train_sample_count,sample_size,num_of_features))\n",
    "X_test = X_test.reshape((test_sample_count,sample_size,num_of_features))\n",
    "\n",
    "print(\"X_train.shape after reshape:\",X_train.shape)\n",
    "print(\"X_validate.shape after reshape:\",X_validate.shape)\n",
    "print(\"X_test.shape after reshape:\",X_test.shape)\n",
    "\n",
    "#collapse y_train and y_test to the same X sample counts instead\n",
    "\n",
    "y_train_collapsed = np.array([])\n",
    "for i in range(len(y_train)):\n",
    "    if (i % sample_size == 0):\n",
    "        y_train_collapsed = np.append(y_train_collapsed, (y_train[i]))\n",
    "        \n",
    "print(\"y_train_collapsed shape:\",y_train_collapsed.shape)        \n",
    "\n",
    "y_validate_collapsed = np.array([])\n",
    "for i in range(len(y_validate)):\n",
    "    if (i % sample_size == 0):\n",
    "        y_validate_collapsed = np.append(y_validate_collapsed, (y_validate[i]))\n",
    "        \n",
    "print(\"y_validate_collapsed shape:\",y_validate_collapsed.shape)\n",
    "\n",
    "y_test_collapsed = np.array([])\n",
    "for i in range(len(y_test)):\n",
    "    if (i % sample_size == 0):\n",
    "        y_test_collapsed = np.append(y_test_collapsed, (y_test[i]))\n",
    "        \n",
    "print(\"y_test_collapsed shape:\",y_test_collapsed.shape)    \n",
    "\n",
    "\n",
    "y_train = pd.get_dummies(y_train_collapsed)\n",
    "y_validate = pd.get_dummies(y_validate_collapsed)\n",
    "y_test = pd.get_dummies(y_test_collapsed)\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_validate.shape:\", y_validate.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "\n",
    "print(\"y_train:\")\n",
    "print(y_train[:5])\n",
    "\n",
    "#######\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "validate_dataset = tf.data.Dataset.from_tensor_slices((X_validate, y_validate))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "train_dataset.with_options(options)\n",
    "validate_dataset.with_options(options)\n",
    "test_dataset.with_options(options)\n",
    "\n",
    "batch_size = 30\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "validate_dataset = validate_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a03033",
   "metadata": {},
   "source": [
    "<h3>Manual runs to get a feel of the hyperparameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12854c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=True, optimizer=\"SGD\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50],\n",
    "                     conv_kernel_size_array=[3],\n",
    "                     conv_strides_array=[1]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=True, optimizer=\"SGD\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50],\n",
    "                     conv_kernel_size_array=[3,3],\n",
    "                     conv_strides_array=[1,1]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19386f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=True, optimizer=\"SGD\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50,50],\n",
    "                     conv_kernel_size_array=[3,3,3],\n",
    "                     conv_strides_array=[1,1,1]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9861925",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(train_dataset, validate_dataset, X_train, save_to= './', epoch = 40, \n",
    "                     dense_layers_unit_array=[], learningRate=0.1,\n",
    "                     startWithBatchNormalized=True, optimizer=\"SGD\",dropOutRate=0.5,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50,50,50],\n",
    "                     conv_kernel_size_array=[3,3,3,3],\n",
    "                     conv_strides_array=[1,1,1,1]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60946966",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_model(train_dataset, validate_dataset, X_train, save_to= './', epoch = 80, \n",
    "                     dense_layers_unit_array=[], learningRate=0.001,\n",
    "                     startWithBatchNormalized=False, optimizer=\"Adam\",dropOutRate=0.2,\n",
    "                     activation=\"relu\",\n",
    "                     conv_layers_filters_array=[50,50,50,50],\n",
    "                     conv_kernel_size_array=[3,3,3,3],\n",
    "                     conv_strides_array=[1,1,1,1],\n",
    "                     patience=20\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87235f7",
   "metadata": {},
   "source": [
    "<h3>Perform hyperparam tuning using the CNN layer(s)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_features = X_train.shape[1]\n",
    "\n",
    "def model_tuner(hp):\n",
    "    print(\"number_of_features:\", number_of_features)\n",
    "    \n",
    "    # don't add any dense layer\n",
    "    dense_layers_unit_array = []\n",
    "    \n",
    "    conv_layers_filters_array=[]\n",
    "    conv_kernel_size_array=[]\n",
    "    conv_strides_array=[]\n",
    "    \n",
    "    conv_layers_filters_array_size = hp.Int('conv_layers_filters_array_size', min_value=1, max_value=4, step=1)\n",
    "    for i in range(conv_layers_filters_array_size):\n",
    "        conv_layers_filters_array.append(hp.Int('conv_layers_filters_'+str(i), \n",
    "                                                min_value=10, max_value=70, step=10))\n",
    "        conv_kernel_size_array.append(hp.Int('conv_kernel_'+str(i), \n",
    "                                                min_value=2, max_value=5, step=1))\n",
    "        conv_strides_array.append(hp.Int('conv_stride_'+str(i), \n",
    "                                          min_value=1, max_value=3, step=1))\n",
    "    \n",
    "    model = build_model(number_of_features, \n",
    "                dense_layers_unit_array=dense_layers_unit_array, \n",
    "                learningRate=hp.Choice('learningRate', values=[0.01, 0.05, 0.1]),\n",
    "                activation=hp.Choice('activation', values=[\"tanh\", \"relu\"]), \n",
    "                isBatchNormalized=hp.Choice(\"isBatchNormalized\", values=[True, False]), \n",
    "                dropOutRate=hp.Float('dropOutRate', min_value=0, max_value=0.7, step=0.1),\n",
    "                startWithBatchNormalized=hp.Choice(\"startWithBatchNormalized\", values=[True, False]),      \n",
    "                optimizer=hp.Choice(\"optimizer\", values=[\"Adam\", \"SGD\"]),\n",
    "                conv_layers_filters_array=conv_layers_filters_array,\n",
    "                conv_kernel_size_array=conv_kernel_size_array,\n",
    "                conv_strides_array=conv_strides_array     \n",
    "                       )\n",
    "    \n",
    "    return model\n",
    "                        \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "epochs = 40\n",
    "tuner = kt.Hyperband(model_tuner,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=epochs,\n",
    "                     factor=3,\n",
    "                     directory='.',\n",
    "                     seed=1234,\n",
    "                     project_name='eeg')\n",
    "\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "\n",
    "tuner.search(train_dataset, \n",
    "             epochs=epochs, \n",
    "             validation_data=validate_dataset,\n",
    "             shuffle=True,\n",
    "             callbacks=[stop_early])\n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"#### Hypertuning results:\")\n",
    "\n",
    "print(\"learningRate:\",best_hps.get('learningRate'))\n",
    "print(\"isBatchNormalized:\",best_hps.get('isBatchNormalized'))\n",
    "print(\"dropOutRate:\",best_hps.get('dropOutRate'))\n",
    "print(\"conv_layers_filters_array_size:\",best_hps.get('conv_layers_filters_array_size'))\n",
    "\n",
    "try:\n",
    "    print(\"conv_layers_filters_0:\",best_hps.get('conv_layers_filters_0'))\n",
    "    print(\"conv_kernel_0:\",best_hps.get('conv_kernel_0'))\n",
    "    print(\"conv_stride_0:\",best_hps.get('conv_stride_0'))\n",
    "except:\n",
    "    print(\"no conv layer 0\")\n",
    "    \n",
    "try:\n",
    "    print(\"conv_layers_filters_1:\",best_hps.get('conv_layers_filters_1'))\n",
    "    print(\"conv_kernel_1:\",best_hps.get('conv_kernel_1'))\n",
    "    print(\"conv_stride_1:\",best_hps.get('conv_stride_1'))\n",
    "except:\n",
    "    print(\"no conv layer 1\")\n",
    "    \n",
    "try:\n",
    "    print(\"conv_layers_filters_2:\",best_hps.get('conv_layers_filters_2'))\n",
    "    print(\"conv_kernel_2:\",best_hps.get('conv_kernel_2'))\n",
    "    print(\"conv_stride_2:\",best_hps.get('conv_stride_2'))    \n",
    "except:\n",
    "    print(\"no conv layer 2\")\n",
    "    \n",
    "try:\n",
    "    print(\"conv_layers_filters_0:\",best_hps.get('conv_layers_filters_3'))\n",
    "    print(\"conv_kernel_3:\",best_hps.get('conv_kernel_3'))\n",
    "    print(\"conv_stride_3:\",best_hps.get('conv_stride_3'))    \n",
    "except:\n",
    "    print(\"no conv layer 3\")    \n",
    "\n",
    "print(\"dropOutRate:\",best_hps.get('dropOutRate'))\n",
    "\n",
    "print(\"#########################################\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(train_dataset, \n",
    "             epochs=epochs, \n",
    "             validation_data=validate_dataset,\n",
    "             shuffle=True,\n",
    "             callbacks=[stop_early])\n",
    "\n",
    "history_data = pd.DataFrame(history.history)\n",
    "plot_loss_history(history_data)\n",
    "plot_accuracy_history(history_data)\n",
    "\n",
    "# test model\n",
    "test_results = model.evaluate(test_dataset)\n",
    "print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))\n",
    "\n",
    "# show classification report\n",
    "y_predict = np.array(model.predict(test_dataset))\n",
    "y_predict = to_categorical(np.argmax(y_predict, axis=1), 4)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9da61",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "\n",
    "Hyperparam tuning on CNN:  accuracy </br>\n",
    "\n",
    "\n",
    "Manual tuning on CNN:  accuracy </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50945e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
