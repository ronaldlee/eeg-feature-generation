{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b2598b-17dc-4af2-8fb6-8718e19d1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b893b8-cd48-4fcf-a9cd-26ebbf7e7d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_4</th>\n",
       "      <th>lag1_mean_5</th>\n",
       "      <th>lag1_mean_6</th>\n",
       "      <th>lag1_mean_7</th>\n",
       "      <th>lag1_mean_8</th>\n",
       "      <th>lag1_mean_9</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_669_13</th>\n",
       "      <th>freq_679_13</th>\n",
       "      <th>freq_689_13</th>\n",
       "      <th>freq_699_13</th>\n",
       "      <th>freq_709_13</th>\n",
       "      <th>freq_720_13</th>\n",
       "      <th>freq_730_13</th>\n",
       "      <th>freq_740_13</th>\n",
       "      <th>freq_750_13</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.934765</td>\n",
       "      <td>0.267884</td>\n",
       "      <td>-2.144542</td>\n",
       "      <td>-2.533547</td>\n",
       "      <td>-3.066073</td>\n",
       "      <td>0.328303</td>\n",
       "      <td>-1.131894</td>\n",
       "      <td>5.429830</td>\n",
       "      <td>-0.836372</td>\n",
       "      <td>8.041636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.458215</td>\n",
       "      <td>5.005935</td>\n",
       "      <td>-0.725545</td>\n",
       "      <td>8.512712</td>\n",
       "      <td>-17.244226</td>\n",
       "      <td>20.164538</td>\n",
       "      <td>-0.707510</td>\n",
       "      <td>1.786132</td>\n",
       "      <td>-1.287160</td>\n",
       "      <td>-1.528923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.055457</td>\n",
       "      <td>-0.677152</td>\n",
       "      <td>0.241560</td>\n",
       "      <td>0.079234</td>\n",
       "      <td>-1.386140</td>\n",
       "      <td>-0.207097</td>\n",
       "      <td>-0.201138</td>\n",
       "      <td>-5.107441</td>\n",
       "      <td>1.374121</td>\n",
       "      <td>-0.398227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.004879</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.104418</td>\n",
       "      <td>0.758306</td>\n",
       "      <td>0.877612</td>\n",
       "      <td>1.910334</td>\n",
       "      <td>5.005314</td>\n",
       "      <td>3.530473</td>\n",
       "      <td>-1.477243</td>\n",
       "      <td>0.118954</td>\n",
       "      <td>-0.916840</td>\n",
       "      <td>-1.358008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.005344</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.543269</td>\n",
       "      <td>-0.106916</td>\n",
       "      <td>0.183276</td>\n",
       "      <td>-0.624663</td>\n",
       "      <td>0.278018</td>\n",
       "      <td>-0.392274</td>\n",
       "      <td>-0.944821</td>\n",
       "      <td>1.830702</td>\n",
       "      <td>-0.935904</td>\n",
       "      <td>2.064984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003381</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.010865</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3739 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
       "0    -2.934765     0.267884    -2.144542    -2.533547    -3.066073   \n",
       "1    -6.458215     5.005935    -0.725545     8.512712   -17.244226   \n",
       "2    -1.055457    -0.677152     0.241560     0.079234    -1.386140   \n",
       "3     1.104418     0.758306     0.877612     1.910334     5.005314   \n",
       "4     1.543269    -0.106916     0.183276    -0.624663     0.278018   \n",
       "\n",
       "   lag1_mean_5  lag1_mean_6  lag1_mean_7  lag1_mean_8  lag1_mean_9  ...  \\\n",
       "0     0.328303    -1.131894     5.429830    -0.836372     8.041636  ...   \n",
       "1    20.164538    -0.707510     1.786132    -1.287160    -1.528923  ...   \n",
       "2    -0.207097    -0.201138    -5.107441     1.374121    -0.398227  ...   \n",
       "3     3.530473    -1.477243     0.118954    -0.916840    -1.358008  ...   \n",
       "4    -0.392274    -0.944821     1.830702    -0.935904     2.064984  ...   \n",
       "\n",
       "   freq_669_13  freq_679_13  freq_689_13  freq_699_13  freq_709_13  \\\n",
       "0     0.007022     0.009016     0.002528     0.004360     0.002872   \n",
       "1     0.007777     0.003482     0.003310     0.001004     0.001927   \n",
       "2     0.006512     0.004879     0.004725     0.003842     0.003871   \n",
       "3     0.006753     0.008789     0.005344     0.000313     0.002932   \n",
       "4     0.003381     0.001809     0.002519     0.002780     0.010865   \n",
       "\n",
       "   freq_720_13  freq_730_13  freq_740_13  freq_750_13  Label  \n",
       "0     0.002023     0.004519     0.003357     0.004046    4.0  \n",
       "1     0.004033     0.001630     0.002139     0.002550    1.0  \n",
       "2     0.002725     0.002234     0.002157     0.000697    1.0  \n",
       "3     0.003106     0.004528     0.003814     0.003689    2.0  \n",
       "4     0.003386     0.002724     0.007896     0.007865    4.0  \n",
       "\n",
       "[5 rows x 3739 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_df = pd.read_csv('csv/out_gameemo.csv', sep=',')\n",
    "init_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0897dea4-7fc4-4de7-addc-6c32e77f7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_labels = {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3}\n",
    "init_df[\"Label\"] = init_df[\"Label\"].map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b4346c-e511-4048-8440-20bb4c9dc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = init_df.iloc[:,:-1]\n",
    "y = init_df.iloc[:,-1:] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6631cb-1ffb-4eb0-8fd8-e8d7709beb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (40071, 3739)\n",
      "train features.shape:  (28049, 3738)\n",
      "train label.shape:  (28049, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data: ', init_df.shape)\n",
    "print('train features.shape: ', X_train.shape)\n",
    "print('train label.shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce34c30-d8a3-47cc-bbc3-af1b3d4ee2c5",
   "metadata": {},
   "source": [
    "# First decide the best amount of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38351a39-94c0-4b45-acee-cfe8283f334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.814325 using {'hl_1_nodes': 64, 'hl_2_nodes': 16, 'hl_3': False}\n",
      "0.780919 (0.009826) with: {'hl_1_nodes': 16, 'hl_2_nodes': 0, 'hl_3': True}\n",
      "0.792328 (0.005924) with: {'hl_1_nodes': 16, 'hl_2_nodes': 0, 'hl_3': False}\n",
      "0.778673 (0.008512) with: {'hl_1_nodes': 16, 'hl_2_nodes': 16, 'hl_3': True}\n",
      "0.785340 (0.000157) with: {'hl_1_nodes': 16, 'hl_2_nodes': 16, 'hl_3': False}\n",
      "0.769118 (0.014151) with: {'hl_1_nodes': 16, 'hl_2_nodes': 32, 'hl_3': True}\n",
      "0.788156 (0.000654) with: {'hl_1_nodes': 16, 'hl_2_nodes': 32, 'hl_3': False}\n",
      "0.782773 (0.007986) with: {'hl_1_nodes': 16, 'hl_2_nodes': 64, 'hl_3': True}\n",
      "0.791258 (0.001270) with: {'hl_1_nodes': 16, 'hl_2_nodes': 64, 'hl_3': False}\n",
      "0.801205 (0.004299) with: {'hl_1_nodes': 32, 'hl_2_nodes': 0, 'hl_3': True}\n",
      "0.807515 (0.003628) with: {'hl_1_nodes': 32, 'hl_2_nodes': 0, 'hl_3': False}\n",
      "0.789226 (0.007043) with: {'hl_1_nodes': 32, 'hl_2_nodes': 16, 'hl_3': True}\n",
      "0.801918 (0.002263) with: {'hl_1_nodes': 32, 'hl_2_nodes': 16, 'hl_3': False}\n",
      "0.790866 (0.004814) with: {'hl_1_nodes': 32, 'hl_2_nodes': 32, 'hl_3': True}\n",
      "0.806125 (0.002799) with: {'hl_1_nodes': 32, 'hl_2_nodes': 32, 'hl_3': False}\n",
      "0.799708 (0.003451) with: {'hl_1_nodes': 32, 'hl_2_nodes': 64, 'hl_3': True}\n",
      "0.805270 (0.006676) with: {'hl_1_nodes': 32, 'hl_2_nodes': 64, 'hl_3': False}\n",
      "0.805555 (0.000449) with: {'hl_1_nodes': 64, 'hl_2_nodes': 0, 'hl_3': True}\n",
      "0.810938 (0.003723) with: {'hl_1_nodes': 64, 'hl_2_nodes': 0, 'hl_3': False}\n",
      "0.804806 (0.003632) with: {'hl_1_nodes': 64, 'hl_2_nodes': 16, 'hl_3': True}\n",
      "0.814325 (0.003458) with: {'hl_1_nodes': 64, 'hl_2_nodes': 16, 'hl_3': False}\n",
      "0.806303 (0.002958) with: {'hl_1_nodes': 64, 'hl_2_nodes': 32, 'hl_3': True}\n",
      "0.811045 (0.000970) with: {'hl_1_nodes': 64, 'hl_2_nodes': 32, 'hl_3': False}\n",
      "0.798852 (0.002627) with: {'hl_1_nodes': 64, 'hl_2_nodes': 64, 'hl_3': True}\n",
      "0.808015 (0.001233) with: {'hl_1_nodes': 64, 'hl_2_nodes': 64, 'hl_3': False}\n"
     ]
    }
   ],
   "source": [
    "def create_model(hl_1_nodes=16,hl_2_nodes=0,hl_3=False):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(units=hl_1_nodes,activation='relu'))\n",
    "\n",
    "    if hl_2_nodes > 0:\n",
    "        model.add(Dense(units=hl_2_nodes,activation='relu'))\n",
    "   \n",
    "    if hl_3 == True:\n",
    "        model.add(Dense(units=8,activation='relu'))\n",
    "        \n",
    "    model.add(Dense(units=4,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "model = KerasClassifier(model=create_model,epochs=10,batch_size=60,hl_1_nodes=16,hl_2_nodes=0,hl_3=False,verbose=0)\n",
    "\n",
    "hl_1_nodes = [16,32,64]\n",
    "hl_2_nodes = [0,16,32,64]\n",
    "hl_3 = [True,False]\n",
    "param_grid = dict(hl_1_nodes=hl_1_nodes, hl_2_nodes=hl_2_nodes, hl_3=hl_3)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58d1ff-479e-45c8-97c3-8371023b8a73",
   "metadata": {},
   "source": [
    "## All of the top three performing combinations had 64 nodes in the first hidden layer. In almost all cases, the addition of a third layer did not improve the model. Let's try some more combinations for two hidden layers max.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "780a2899-3a4d-41ba-9618-1dd6d4f6888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.819459 using {'hl_1_nodes': 512, 'hl_2_nodes': 64}\n",
      "0.812614 (0.001232) with: {'hl_1_nodes': 64, 'hl_2_nodes': 0}\n",
      "0.814717 (0.006604) with: {'hl_1_nodes': 64, 'hl_2_nodes': 16}\n",
      "0.805162 (0.000714) with: {'hl_1_nodes': 64, 'hl_2_nodes': 32}\n",
      "0.815216 (0.004121) with: {'hl_1_nodes': 64, 'hl_2_nodes': 64}\n",
      "0.811829 (0.005375) with: {'hl_1_nodes': 64, 'hl_2_nodes': 128}\n",
      "0.808835 (0.001660) with: {'hl_1_nodes': 64, 'hl_2_nodes': 256}\n",
      "0.804414 (0.001604) with: {'hl_1_nodes': 64, 'hl_2_nodes': 512}\n",
      "0.808228 (0.001763) with: {'hl_1_nodes': 64, 'hl_2_nodes': 1024}\n",
      "0.808300 (0.003057) with: {'hl_1_nodes': 128, 'hl_2_nodes': 0}\n",
      "0.815822 (0.000639) with: {'hl_1_nodes': 128, 'hl_2_nodes': 16}\n",
      "0.816072 (0.002363) with: {'hl_1_nodes': 128, 'hl_2_nodes': 32}\n",
      "0.813719 (0.001402) with: {'hl_1_nodes': 128, 'hl_2_nodes': 64}\n",
      "0.813362 (0.002595) with: {'hl_1_nodes': 128, 'hl_2_nodes': 128}\n",
      "0.810902 (0.003531) with: {'hl_1_nodes': 128, 'hl_2_nodes': 256}\n",
      "0.814432 (0.003603) with: {'hl_1_nodes': 128, 'hl_2_nodes': 512}\n",
      "0.808371 (0.001489) with: {'hl_1_nodes': 128, 'hl_2_nodes': 1024}\n",
      "0.810475 (0.002450) with: {'hl_1_nodes': 256, 'hl_2_nodes': 0}\n",
      "0.811580 (0.004336) with: {'hl_1_nodes': 256, 'hl_2_nodes': 16}\n",
      "0.815359 (0.001313) with: {'hl_1_nodes': 256, 'hl_2_nodes': 32}\n",
      "0.818354 (0.001107) with: {'hl_1_nodes': 256, 'hl_2_nodes': 64}\n",
      "0.817070 (0.001653) with: {'hl_1_nodes': 256, 'hl_2_nodes': 128}\n",
      "0.814931 (0.001506) with: {'hl_1_nodes': 256, 'hl_2_nodes': 256}\n",
      "0.813434 (0.001977) with: {'hl_1_nodes': 256, 'hl_2_nodes': 512}\n",
      "0.812079 (0.006826) with: {'hl_1_nodes': 256, 'hl_2_nodes': 1024}\n",
      "0.812863 (0.001749) with: {'hl_1_nodes': 512, 'hl_2_nodes': 0}\n",
      "0.815395 (0.002028) with: {'hl_1_nodes': 512, 'hl_2_nodes': 16}\n",
      "0.815787 (0.001695) with: {'hl_1_nodes': 512, 'hl_2_nodes': 32}\n",
      "0.819459 (0.002139) with: {'hl_1_nodes': 512, 'hl_2_nodes': 64}\n",
      "0.814325 (0.000713) with: {'hl_1_nodes': 512, 'hl_2_nodes': 128}\n",
      "0.815252 (0.004018) with: {'hl_1_nodes': 512, 'hl_2_nodes': 256}\n",
      "0.814753 (0.003712) with: {'hl_1_nodes': 512, 'hl_2_nodes': 512}\n",
      "0.813148 (0.000686) with: {'hl_1_nodes': 512, 'hl_2_nodes': 1024}\n"
     ]
    }
   ],
   "source": [
    "def create_model(hl_1_nodes=16,hl_2_nodes=0):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(units=hl_1_nodes,activation='relu'))\n",
    "\n",
    "    if hl_2_nodes > 0:\n",
    "        model.add(Dense(units=hl_2_nodes,activation='relu'))\n",
    "        \n",
    "    model.add(Dense(units=4,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "model = KerasClassifier(model=create_model,epochs=10,batch_size=60,hl_1_nodes=16,hl_2_nodes=0,verbose=0)\n",
    "\n",
    "hl_1_nodes = [64,128,256,512]\n",
    "hl_2_nodes = [0,16,32,64,128,256,512,1024]\n",
    "param_grid = dict(hl_1_nodes=hl_1_nodes, hl_2_nodes=hl_2_nodes)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd59519-24a9-4f18-b9be-7e50ee712278",
   "metadata": {},
   "source": [
    "## The combination of hl_1_nodes = 512 and hl_2_nodes = 64 did the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa0b685-5ea9-4004-bdcd-d6fd1382c4b4",
   "metadata": {},
   "source": [
    "# Find the best optimization algorithm and batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95ddbfd9-03bd-4009-afd3-d89e41e7c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.839709 using {'batch_size': 30, 'optimizer': 'Adam'}\n",
      "0.832115 (0.014021) with: {'batch_size': 30, 'optimizer': 'SGD'}\n",
      "0.839709 (0.004691) with: {'batch_size': 30, 'optimizer': 'Adam'}\n",
      "0.817177 (0.001327) with: {'batch_size': 60, 'optimizer': 'SGD'}\n",
      "0.819530 (0.003172) with: {'batch_size': 60, 'optimizer': 'Adam'}\n",
      "0.779992 (0.001742) with: {'batch_size': 120, 'optimizer': 'SGD'}\n",
      "0.785875 (0.003426) with: {'batch_size': 120, 'optimizer': 'Adam'}\n",
      "0.722878 (0.002540) with: {'batch_size': 240, 'optimizer': 'SGD'}\n",
      "0.720311 (0.001632) with: {'batch_size': 240, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='SGD'):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(units=512,activation='relu'))\n",
    "    model.add(Dense(units=64,activation='relu'))\n",
    "        \n",
    "    model.add(Dense(units=4,activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "model = KerasClassifier(model=create_model,epochs=10,batch_size=60,optimizer='SGD',verbose=0)\n",
    "\n",
    "batch_size = [30,60,120,240]\n",
    "optimizer = ['SGD','Adam']\n",
    "param_grid = dict(batch_size=batch_size, optimizer=optimizer)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5e531-8ecb-4c48-a7d6-f7dd3ec3ba4e",
   "metadata": {},
   "source": [
    "## Batch size of 30 with Adam optimizer performing best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6b8ee-869a-4e88-849d-bf110137a9ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build model with many epochs. Then evaluate on test data and DREAMER data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7291983d-74c4-458a-9038-cfe649c1a018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "842/842 [==============================] - 13s 14ms/step - loss: 0.7199 - accuracy: 0.7041 - val_loss: 0.4981 - val_accuracy: 0.8111\n",
      "Epoch 2/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.3308 - accuracy: 0.8730 - val_loss: 0.3798 - val_accuracy: 0.8620\n",
      "Epoch 3/60\n",
      "842/842 [==============================] - 10s 12ms/step - loss: 0.2011 - accuracy: 0.9264 - val_loss: 0.3448 - val_accuracy: 0.8863\n",
      "Epoch 4/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.1428 - accuracy: 0.9482 - val_loss: 0.3005 - val_accuracy: 0.8955\n",
      "Epoch 5/60\n",
      "842/842 [==============================] - 10s 12ms/step - loss: 0.1098 - accuracy: 0.9617 - val_loss: 0.2955 - val_accuracy: 0.9098\n",
      "Epoch 6/60\n",
      "842/842 [==============================] - 11s 12ms/step - loss: 0.0893 - accuracy: 0.9691 - val_loss: 0.2593 - val_accuracy: 0.9162\n",
      "Epoch 7/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0712 - accuracy: 0.9765 - val_loss: 0.2864 - val_accuracy: 0.9155\n",
      "Epoch 8/60\n",
      "842/842 [==============================] - 11s 12ms/step - loss: 0.0691 - accuracy: 0.9771 - val_loss: 0.2632 - val_accuracy: 0.9234\n",
      "Epoch 9/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0583 - accuracy: 0.9807 - val_loss: 0.2641 - val_accuracy: 0.9262\n",
      "Epoch 10/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0473 - accuracy: 0.9839 - val_loss: 0.2543 - val_accuracy: 0.9291\n",
      "Epoch 11/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0516 - accuracy: 0.9836 - val_loss: 0.2517 - val_accuracy: 0.9301\n",
      "Epoch 12/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0298 - accuracy: 0.9908 - val_loss: 0.2604 - val_accuracy: 0.9422\n",
      "Epoch 13/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0419 - accuracy: 0.9876 - val_loss: 0.2163 - val_accuracy: 0.9444\n",
      "Epoch 14/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0377 - accuracy: 0.9880 - val_loss: 0.2652 - val_accuracy: 0.9330\n",
      "Epoch 15/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0302 - accuracy: 0.9898 - val_loss: 0.2384 - val_accuracy: 0.9412\n",
      "Epoch 16/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0288 - accuracy: 0.9899 - val_loss: 0.4100 - val_accuracy: 0.9127\n",
      "Epoch 17/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0385 - accuracy: 0.9893 - val_loss: 0.2242 - val_accuracy: 0.9426\n",
      "Epoch 18/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0210 - accuracy: 0.9936 - val_loss: 0.2113 - val_accuracy: 0.9469\n",
      "Epoch 19/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0294 - accuracy: 0.9913 - val_loss: 0.2039 - val_accuracy: 0.9458\n",
      "Epoch 20/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0255 - accuracy: 0.9924 - val_loss: 0.2339 - val_accuracy: 0.9430\n",
      "Epoch 21/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0310 - accuracy: 0.9913 - val_loss: 0.2226 - val_accuracy: 0.9430\n",
      "Epoch 22/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.2110 - val_accuracy: 0.9487\n",
      "Epoch 23/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0165 - accuracy: 0.9951 - val_loss: 0.2271 - val_accuracy: 0.9487\n",
      "Epoch 24/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0257 - accuracy: 0.9929 - val_loss: 0.2175 - val_accuracy: 0.9469\n",
      "Epoch 25/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0221 - accuracy: 0.9938 - val_loss: 0.2189 - val_accuracy: 0.9537\n",
      "Epoch 26/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0197 - accuracy: 0.9949 - val_loss: 0.1825 - val_accuracy: 0.9579\n",
      "Epoch 27/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0196 - accuracy: 0.9946 - val_loss: 0.2289 - val_accuracy: 0.9487\n",
      "Epoch 28/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.2221 - val_accuracy: 0.9483\n",
      "Epoch 29/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0257 - accuracy: 0.9935 - val_loss: 0.2279 - val_accuracy: 0.9480\n",
      "Epoch 30/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.3009 - val_accuracy: 0.9408\n",
      "Epoch 31/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0188 - accuracy: 0.9945 - val_loss: 0.2887 - val_accuracy: 0.9465\n",
      "Epoch 32/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0134 - accuracy: 0.9958 - val_loss: 0.2735 - val_accuracy: 0.9430\n",
      "Epoch 33/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0214 - accuracy: 0.9945 - val_loss: 0.2254 - val_accuracy: 0.9480\n",
      "Epoch 34/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0122 - accuracy: 0.9967 - val_loss: 0.2379 - val_accuracy: 0.9472\n",
      "Epoch 35/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0159 - accuracy: 0.9957 - val_loss: 0.1970 - val_accuracy: 0.9544\n",
      "Epoch 36/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0157 - accuracy: 0.9960 - val_loss: 0.2593 - val_accuracy: 0.9451\n",
      "Epoch 37/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.2537 - val_accuracy: 0.9476\n",
      "Epoch 38/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0139 - accuracy: 0.9962 - val_loss: 0.2319 - val_accuracy: 0.9508\n",
      "Epoch 39/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.2208 - val_accuracy: 0.9490\n",
      "Epoch 40/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0149 - accuracy: 0.9958 - val_loss: 0.2554 - val_accuracy: 0.9437\n",
      "Epoch 41/60\n",
      "842/842 [==============================] - 11s 14ms/step - loss: 0.0179 - accuracy: 0.9952 - val_loss: 0.2366 - val_accuracy: 0.9519\n",
      "Epoch 42/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0129 - accuracy: 0.9966 - val_loss: 0.2150 - val_accuracy: 0.9572\n",
      "Epoch 43/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0102 - accuracy: 0.9975 - val_loss: 0.2786 - val_accuracy: 0.9526\n",
      "Epoch 44/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.2419 - val_accuracy: 0.9529\n",
      "Epoch 45/60\n",
      "842/842 [==============================] - 12s 14ms/step - loss: 0.0142 - accuracy: 0.9962 - val_loss: 0.3382 - val_accuracy: 0.9398\n",
      "Epoch 46/60\n",
      "842/842 [==============================] - 12s 14ms/step - loss: 0.0201 - accuracy: 0.9955 - val_loss: 0.2251 - val_accuracy: 0.9544\n",
      "Epoch 47/60\n",
      "842/842 [==============================] - 12s 14ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.2484 - val_accuracy: 0.9540\n",
      "Epoch 48/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0123 - accuracy: 0.9966 - val_loss: 0.2209 - val_accuracy: 0.9533\n",
      "Epoch 49/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.2515 - val_accuracy: 0.9504\n",
      "Epoch 50/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.2117 - val_accuracy: 0.9572\n",
      "Epoch 51/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0150 - accuracy: 0.9964 - val_loss: 0.2620 - val_accuracy: 0.9490\n",
      "Epoch 52/60\n",
      "842/842 [==============================] - 11s 14ms/step - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.2380 - val_accuracy: 0.9508\n",
      "Epoch 53/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.2334 - val_accuracy: 0.9544\n",
      "Epoch 54/60\n",
      "842/842 [==============================] - 11s 14ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.1868 - val_accuracy: 0.9626\n",
      "Epoch 55/60\n",
      "842/842 [==============================] - 12s 14ms/step - loss: 0.0105 - accuracy: 0.9981 - val_loss: 0.2131 - val_accuracy: 0.9565\n",
      "Epoch 56/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.2392 - val_accuracy: 0.9551\n",
      "Epoch 57/60\n",
      "842/842 [==============================] - 11s 13ms/step - loss: 0.0133 - accuracy: 0.9962 - val_loss: 0.2163 - val_accuracy: 0.9576\n",
      "Epoch 58/60\n",
      "842/842 [==============================] - 12s 14ms/step - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2422 - val_accuracy: 0.9451\n",
      "Epoch 59/60\n",
      "842/842 [==============================] - 12s 14ms/step - loss: 0.0147 - accuracy: 0.9973 - val_loss: 0.2894 - val_accuracy: 0.9501\n",
      "Epoch 60/60\n",
      "842/842 [==============================] - 13s 15ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.2293 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.719902</td>\n",
       "      <td>0.704088</td>\n",
       "      <td>0.498141</td>\n",
       "      <td>0.811052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.330808</td>\n",
       "      <td>0.873039</td>\n",
       "      <td>0.379804</td>\n",
       "      <td>0.862032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.201142</td>\n",
       "      <td>0.926359</td>\n",
       "      <td>0.344822</td>\n",
       "      <td>0.886275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.142780</td>\n",
       "      <td>0.948225</td>\n",
       "      <td>0.300497</td>\n",
       "      <td>0.895544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.961733</td>\n",
       "      <td>0.295537</td>\n",
       "      <td>0.909804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.089323</td>\n",
       "      <td>0.969141</td>\n",
       "      <td>0.259276</td>\n",
       "      <td>0.916221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.976509</td>\n",
       "      <td>0.286394</td>\n",
       "      <td>0.915508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.069079</td>\n",
       "      <td>0.977143</td>\n",
       "      <td>0.263242</td>\n",
       "      <td>0.923351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.058270</td>\n",
       "      <td>0.980748</td>\n",
       "      <td>0.264065</td>\n",
       "      <td>0.926203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.047308</td>\n",
       "      <td>0.983877</td>\n",
       "      <td>0.254290</td>\n",
       "      <td>0.929055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.051553</td>\n",
       "      <td>0.983600</td>\n",
       "      <td>0.251650</td>\n",
       "      <td>0.930125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.990770</td>\n",
       "      <td>0.260391</td>\n",
       "      <td>0.942246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.041934</td>\n",
       "      <td>0.987641</td>\n",
       "      <td>0.216271</td>\n",
       "      <td>0.944385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.037690</td>\n",
       "      <td>0.987997</td>\n",
       "      <td>0.265168</td>\n",
       "      <td>0.932977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.030249</td>\n",
       "      <td>0.989819</td>\n",
       "      <td>0.238439</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.028797</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.410019</td>\n",
       "      <td>0.912656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.038466</td>\n",
       "      <td>0.989344</td>\n",
       "      <td>0.224156</td>\n",
       "      <td>0.942603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0.211294</td>\n",
       "      <td>0.946881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.029388</td>\n",
       "      <td>0.991325</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.945811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.025518</td>\n",
       "      <td>0.992394</td>\n",
       "      <td>0.233906</td>\n",
       "      <td>0.942959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.030970</td>\n",
       "      <td>0.991325</td>\n",
       "      <td>0.222625</td>\n",
       "      <td>0.942959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.020031</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.211004</td>\n",
       "      <td>0.948663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.995128</td>\n",
       "      <td>0.227056</td>\n",
       "      <td>0.948663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.025725</td>\n",
       "      <td>0.992870</td>\n",
       "      <td>0.217508</td>\n",
       "      <td>0.946881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.022120</td>\n",
       "      <td>0.993781</td>\n",
       "      <td>0.218916</td>\n",
       "      <td>0.953654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.994929</td>\n",
       "      <td>0.182524</td>\n",
       "      <td>0.957932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.019618</td>\n",
       "      <td>0.994573</td>\n",
       "      <td>0.228903</td>\n",
       "      <td>0.948663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.016289</td>\n",
       "      <td>0.995405</td>\n",
       "      <td>0.222061</td>\n",
       "      <td>0.948307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.025703</td>\n",
       "      <td>0.993503</td>\n",
       "      <td>0.227925</td>\n",
       "      <td>0.947950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.015217</td>\n",
       "      <td>0.995405</td>\n",
       "      <td>0.300926</td>\n",
       "      <td>0.940820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.994454</td>\n",
       "      <td>0.288692</td>\n",
       "      <td>0.946524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.995841</td>\n",
       "      <td>0.273462</td>\n",
       "      <td>0.942959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.021361</td>\n",
       "      <td>0.994533</td>\n",
       "      <td>0.225405</td>\n",
       "      <td>0.947950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.012241</td>\n",
       "      <td>0.996712</td>\n",
       "      <td>0.237882</td>\n",
       "      <td>0.947237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.015881</td>\n",
       "      <td>0.995682</td>\n",
       "      <td>0.196999</td>\n",
       "      <td>0.954367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.995959</td>\n",
       "      <td>0.259345</td>\n",
       "      <td>0.945098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.995405</td>\n",
       "      <td>0.253702</td>\n",
       "      <td>0.947594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.996197</td>\n",
       "      <td>0.231932</td>\n",
       "      <td>0.950802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.014847</td>\n",
       "      <td>0.995603</td>\n",
       "      <td>0.220751</td>\n",
       "      <td>0.949020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.014888</td>\n",
       "      <td>0.995801</td>\n",
       "      <td>0.255369</td>\n",
       "      <td>0.943672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.017941</td>\n",
       "      <td>0.995167</td>\n",
       "      <td>0.236577</td>\n",
       "      <td>0.951872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.996593</td>\n",
       "      <td>0.214964</td>\n",
       "      <td>0.957219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.010198</td>\n",
       "      <td>0.997465</td>\n",
       "      <td>0.278551</td>\n",
       "      <td>0.952585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.015993</td>\n",
       "      <td>0.995326</td>\n",
       "      <td>0.241887</td>\n",
       "      <td>0.952941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.014191</td>\n",
       "      <td>0.996237</td>\n",
       "      <td>0.338205</td>\n",
       "      <td>0.939750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.020137</td>\n",
       "      <td>0.995524</td>\n",
       "      <td>0.225124</td>\n",
       "      <td>0.954367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.998099</td>\n",
       "      <td>0.248437</td>\n",
       "      <td>0.954011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.012336</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.220924</td>\n",
       "      <td>0.953298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.015645</td>\n",
       "      <td>0.995365</td>\n",
       "      <td>0.251505</td>\n",
       "      <td>0.950446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.011608</td>\n",
       "      <td>0.996989</td>\n",
       "      <td>0.211697</td>\n",
       "      <td>0.957219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.996356</td>\n",
       "      <td>0.262021</td>\n",
       "      <td>0.949020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.996316</td>\n",
       "      <td>0.238004</td>\n",
       "      <td>0.950802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.012239</td>\n",
       "      <td>0.997108</td>\n",
       "      <td>0.233352</td>\n",
       "      <td>0.954367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.997029</td>\n",
       "      <td>0.186777</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.010529</td>\n",
       "      <td>0.998138</td>\n",
       "      <td>0.213147</td>\n",
       "      <td>0.956506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.955080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.996158</td>\n",
       "      <td>0.216266</td>\n",
       "      <td>0.957576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.995841</td>\n",
       "      <td>0.242186</td>\n",
       "      <td>0.945098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.014717</td>\n",
       "      <td>0.997346</td>\n",
       "      <td>0.289396</td>\n",
       "      <td>0.950089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.012275</td>\n",
       "      <td>0.996950</td>\n",
       "      <td>0.229279</td>\n",
       "      <td>0.959358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.719902  0.704088  0.498141      0.811052\n",
       "1   0.330808  0.873039  0.379804      0.862032\n",
       "2   0.201142  0.926359  0.344822      0.886275\n",
       "3   0.142780  0.948225  0.300497      0.895544\n",
       "4   0.109800  0.961733  0.295537      0.909804\n",
       "5   0.089323  0.969141  0.259276      0.916221\n",
       "6   0.071164  0.976509  0.286394      0.915508\n",
       "7   0.069079  0.977143  0.263242      0.923351\n",
       "8   0.058270  0.980748  0.264065      0.926203\n",
       "9   0.047308  0.983877  0.254290      0.929055\n",
       "10  0.051553  0.983600  0.251650      0.930125\n",
       "11  0.029762  0.990770  0.260391      0.942246\n",
       "12  0.041934  0.987641  0.216271      0.944385\n",
       "13  0.037690  0.987997  0.265168      0.932977\n",
       "14  0.030249  0.989819  0.238439      0.941176\n",
       "15  0.028797  0.989899  0.410019      0.912656\n",
       "16  0.038466  0.989344  0.224156      0.942603\n",
       "17  0.020982  0.993622  0.211294      0.946881\n",
       "18  0.029388  0.991325  0.203910      0.945811\n",
       "19  0.025518  0.992394  0.233906      0.942959\n",
       "20  0.030970  0.991325  0.222625      0.942959\n",
       "21  0.020031  0.993939  0.211004      0.948663\n",
       "22  0.016514  0.995128  0.227056      0.948663\n",
       "23  0.025725  0.992870  0.217508      0.946881\n",
       "24  0.022120  0.993781  0.218916      0.953654\n",
       "25  0.019705  0.994929  0.182524      0.957932\n",
       "26  0.019618  0.994573  0.228903      0.948663\n",
       "27  0.016289  0.995405  0.222061      0.948307\n",
       "28  0.025703  0.993503  0.227925      0.947950\n",
       "29  0.015217  0.995405  0.300926      0.940820\n",
       "30  0.018841  0.994454  0.288692      0.946524\n",
       "31  0.013411  0.995841  0.273462      0.942959\n",
       "32  0.021361  0.994533  0.225405      0.947950\n",
       "33  0.012241  0.996712  0.237882      0.947237\n",
       "34  0.015881  0.995682  0.196999      0.954367\n",
       "35  0.015659  0.995959  0.259345      0.945098\n",
       "36  0.016335  0.995405  0.253702      0.947594\n",
       "37  0.013904  0.996197  0.231932      0.950802\n",
       "38  0.014847  0.995603  0.220751      0.949020\n",
       "39  0.014888  0.995801  0.255369      0.943672\n",
       "40  0.017941  0.995167  0.236577      0.951872\n",
       "41  0.012863  0.996593  0.214964      0.957219\n",
       "42  0.010198  0.997465  0.278551      0.952585\n",
       "43  0.015993  0.995326  0.241887      0.952941\n",
       "44  0.014191  0.996237  0.338205      0.939750\n",
       "45  0.020137  0.995524  0.225124      0.954367\n",
       "46  0.005653  0.998099  0.248437      0.954011\n",
       "47  0.012336  0.996633  0.220924      0.953298\n",
       "48  0.015645  0.995365  0.251505      0.950446\n",
       "49  0.011608  0.996989  0.211697      0.957219\n",
       "50  0.015046  0.996356  0.262021      0.949020\n",
       "51  0.012888  0.996316  0.238004      0.950802\n",
       "52  0.012239  0.997108  0.233352      0.954367\n",
       "53  0.013072  0.997029  0.186777      0.962567\n",
       "54  0.010529  0.998138  0.213147      0.956506\n",
       "55  0.006204  0.998217  0.239200      0.955080\n",
       "56  0.013302  0.996158  0.216266      0.957576\n",
       "57  0.018730  0.995841  0.242186      0.945098\n",
       "58  0.014717  0.997346  0.289396      0.950089\n",
       "59  0.012275  0.996950  0.229279      0.959358"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model_a():\n",
    "\n",
    "    model = Sequential([BatchNormalization(),\n",
    "                              Dense(units=512,activation='relu'),\n",
    "                              Dense(units=64,activation='relu'),\n",
    "                              Dense(units=4,activation='softmax')])\n",
    "\n",
    "    optimizer =  tf.keras.optimizers.Adam()\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model_a()\n",
    "\n",
    "history = model.fit(x=X_train,y=y_train,epochs=60,batch_size=30,validation_split=0.1)\n",
    "\n",
    "history = pd.DataFrame(history.history)\n",
    "display(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6053c0a5-587a-42f0-8abe-6b447f8d5d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/376 [==============================] - 1s 4ms/step - loss: 0.2959 - accuracy: 0.9539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2958739697933197, 0.9539178013801575]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3aac4ef-af97-4da0-9fe1-ab926ab2d9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_4</th>\n",
       "      <th>lag1_mean_5</th>\n",
       "      <th>lag1_mean_6</th>\n",
       "      <th>lag1_mean_7</th>\n",
       "      <th>lag1_mean_8</th>\n",
       "      <th>lag1_mean_9</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_669_13</th>\n",
       "      <th>freq_679_13</th>\n",
       "      <th>freq_689_13</th>\n",
       "      <th>freq_699_13</th>\n",
       "      <th>freq_709_13</th>\n",
       "      <th>freq_720_13</th>\n",
       "      <th>freq_730_13</th>\n",
       "      <th>freq_740_13</th>\n",
       "      <th>freq_750_13</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4395.309365</td>\n",
       "      <td>4105.013935</td>\n",
       "      <td>4175.248049</td>\n",
       "      <td>4425.646600</td>\n",
       "      <td>4322.146042</td>\n",
       "      <td>4367.742475</td>\n",
       "      <td>4460.532330</td>\n",
       "      <td>3951.243032</td>\n",
       "      <td>4310.554627</td>\n",
       "      <td>4289.464883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4386.211466</td>\n",
       "      <td>4095.978104</td>\n",
       "      <td>4169.008931</td>\n",
       "      <td>4417.412849</td>\n",
       "      <td>4333.408240</td>\n",
       "      <td>4366.058773</td>\n",
       "      <td>4440.651109</td>\n",
       "      <td>3943.298761</td>\n",
       "      <td>4311.426102</td>\n",
       "      <td>4273.362143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4390.169539</td>\n",
       "      <td>4111.878941</td>\n",
       "      <td>4194.308533</td>\n",
       "      <td>4410.931764</td>\n",
       "      <td>4319.355471</td>\n",
       "      <td>4391.438980</td>\n",
       "      <td>4459.854281</td>\n",
       "      <td>3964.752697</td>\n",
       "      <td>4333.582738</td>\n",
       "      <td>4298.761384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4373.919067</td>\n",
       "      <td>4097.929275</td>\n",
       "      <td>4165.580265</td>\n",
       "      <td>4410.059546</td>\n",
       "      <td>4317.934135</td>\n",
       "      <td>4366.195163</td>\n",
       "      <td>4454.081905</td>\n",
       "      <td>3933.556933</td>\n",
       "      <td>4297.768866</td>\n",
       "      <td>4204.851136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4385.213675</td>\n",
       "      <td>4091.960298</td>\n",
       "      <td>4169.947615</td>\n",
       "      <td>4412.194651</td>\n",
       "      <td>4216.468156</td>\n",
       "      <td>4370.512821</td>\n",
       "      <td>4436.228288</td>\n",
       "      <td>3926.377171</td>\n",
       "      <td>4298.296112</td>\n",
       "      <td>4244.778053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3739 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
       "0  4395.309365  4105.013935  4175.248049  4425.646600  4322.146042   \n",
       "1  4386.211466  4095.978104  4169.008931  4417.412849  4333.408240   \n",
       "2  4390.169539  4111.878941  4194.308533  4410.931764  4319.355471   \n",
       "3  4373.919067  4097.929275  4165.580265  4410.059546  4317.934135   \n",
       "4  4385.213675  4091.960298  4169.947615  4412.194651  4216.468156   \n",
       "\n",
       "   lag1_mean_5  lag1_mean_6  lag1_mean_7  lag1_mean_8  lag1_mean_9  ...  \\\n",
       "0  4367.742475  4460.532330  3951.243032  4310.554627  4289.464883  ...   \n",
       "1  4366.058773  4440.651109  3943.298761  4311.426102  4273.362143  ...   \n",
       "2  4391.438980  4459.854281  3964.752697  4333.582738  4298.761384  ...   \n",
       "3  4366.195163  4454.081905  3933.556933  4297.768866  4204.851136  ...   \n",
       "4  4370.512821  4436.228288  3926.377171  4298.296112  4244.778053  ...   \n",
       "\n",
       "   freq_669_13  freq_679_13  freq_689_13  freq_699_13  freq_709_13  \\\n",
       "0     0.003496     0.003562     0.003310     0.002777     0.003207   \n",
       "1     0.000691     0.000187     0.000287     0.000089     0.000056   \n",
       "2     0.000160     0.000615     0.000067     0.000433     0.000297   \n",
       "3     0.000925     0.001757     0.000949     0.000445     0.000891   \n",
       "4     0.000302     0.000235     0.000631     0.000370     0.000171   \n",
       "\n",
       "   freq_720_13  freq_730_13  freq_740_13  freq_750_13  Label  \n",
       "0     0.003174     0.003147     0.003079     0.002950    4.0  \n",
       "1     0.000134     0.000087     0.000163     0.000147    2.0  \n",
       "2     0.000114     0.000169     0.000115     0.000050    2.0  \n",
       "3     0.003211     0.001094     0.001721     0.002623    3.0  \n",
       "4     0.000019     0.000051     0.000160     0.000142    4.0  \n",
       "\n",
       "[5 rows x 3739 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dream_df = pd.read_csv('out_dreamer.csv', sep=',')\n",
    "dream_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d966381-c4ba-4792-8157-114ef1d25b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_labels = {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3}\n",
    "dream_df[\"Label\"] = dream_df[\"Label\"].map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b929f028-e0f5-4a28-9827-b9d66f7d2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_X = dream_df.iloc[:,:-1]\n",
    "dream_y = dream_df.iloc[:,-1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1a94ac2-54ed-4b4b-b48a-f78eb3badedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3549/3549 [==============================] - 19s 5ms/step - loss: 20.4581 - accuracy: 0.2674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.458141326904297, 0.26743581891059875]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=dream_X,y=dream_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63125a-7c93-4ac5-871b-0eff7159a061",
   "metadata": {},
   "source": [
    "# Making predictions with the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661eef8-0e1e-4da8-9e20-baaf44148893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf9f82-1da9-4da2-84be-789b90e739a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"original_data_with_timestamps\"\n",
    "\n",
    "dirs = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0943e-94f2-489e-acf5-bc952d45ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('original_data_with_timestamps/S03G2AllChannels.csv')\n",
    "testdf.drop(columns='timestamps', inplace=True)\n",
    "testdf = testdf.reindex(columns=['O1','P7','T7','F7','FC5','F3','AF3','AF4','F4','FC6','F8','T8','P8','O2'])\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180f706-e4ce-4905-a1b0-60c49e964618",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_7_sec = []\n",
    "y = []\n",
    "for file in dirs:\n",
    "    df = pd.read_csv('original_data_with_timestamps/' + file)\n",
    "    df.drop(columns='timestamps', inplace=True)\n",
    "    list_of_arrays = np.array_split(df[:37632],42)\n",
    "    for array in list_of_arrays:\n",
    "        X_7_sec.append(array)\n",
    "        y.append(str(file)[4])\n",
    "X_7_sec = np.array(X_7_sec)\n",
    "y = np.array([int(cat)-1 for cat in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443e838-48c3-43b4-a580-997e64f6d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_7_sec, y, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489e696-a010-4816-865b-b09d89b3d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train features.shape: ', Xr_train.shape)\n",
    "print('train label.shape: ', yr_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23e51a-aad1-4673-a3cb-0c4f9ad93236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_raw():\n",
    "\n",
    "    model = Sequential([Conv2D(filters=8, kernel_size=(128,1), strides=(32,1), activation='relu', padding='valid', input_shape=(896,14,1)),\n",
    "                        \n",
    "                        Flatten(),\n",
    "                        \n",
    "                        #BatchNormalization(),\n",
    "                        #Dense(units=256,activation='relu'),\n",
    "                        BatchNormalization(),\n",
    "                        \n",
    "                        Dense(units=4,activation='softmax')])\n",
    "\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.2)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2d4dc-e2a0-4925-9095-d735e7fb96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_raw()\n",
    "\n",
    "history = model.fit(x=Xr_train,y=yr_train,epochs=10,batch_size=60,validation_split=0.1)\n",
    "\n",
    "history = pd.DataFrame(history.history)\n",
    "display(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
