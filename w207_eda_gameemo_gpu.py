# -*- coding: utf-8 -*-
"""w207_eda_gameemo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pviGQntIb_ocbVm0F13Ok-O8zikiXAwU
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


import tensorflow as tf
from tensorflow import keras

from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

from tensorflow.keras.models import Sequential, Model, load_model


#from google.colab import drive
#drive.mount('/content/drive')

#init_df = pd.read_csv('csv/out_gameemo_AF3_AF4.csv',  sep=',')
init_df = pd.read_csv('csv/out_gameemo.csv',  sep=',')
#init_df = pd.read_csv('emotions.csv',  sep=',')
#init_df = pd.read_csv('out_gameemo_F3_F4.csv',  sep=',')
#init_df = pd.read_csv('out_gameemo_F7_F8.csv',  sep=',')
#init_df = pd.read_csv('out_gameemo_FC5_FC6.csv',  sep=',')
#init_df = pd.read_csv('out_gameemo_O1_O2.csv',  sep=',')
#init_df = pd.read_csv('out_gameemo_P7_P8.csv',  sep=',')
#init_df = pd.read_csv('out_gameemo_T7_T8.csv',  sep=',')




print('Shape of data: ', init_df.shape)
print("bb:",type(init_df))

#Filter out label = 2 and 3, only keep 1 (+ve) and 4 (-ve)
#init_df = init_df[ (init_df.Label == 1) | (init_df.Label == 4) ]

#HA_PV = high arousal, positive valence
#HA_NV = high arousal, negative valence
#LA_NV = low arousal, negative valence
#LA_PV = low arousal, positive valance
label_map = {1:"HA_PV", 2:"HA_NV", 3:"LA_NV", 4:"LA_PV"}

init_df["Label"] = init_df["Label"].map(label_map)

print(init_df.head())

#exit()

features = init_df.iloc[:, :-1]
label = init_df.iloc[:, -1:]

print('Shape of data: ', init_df.shape)
print('features.shape: ', features.shape)
print('label.shape: ', label.shape)

init_df.head()
print(init_df.columns)


# check out the frequency data plot
#fft_data = init_df.loc[:,'freq_010_0':'freq_750_12']

#fft_data.iloc[0,:].plot(figsize=(15,10))

#fft_data.iloc[1,:].plot(figsize=(15,10))

y = label
X = features

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=48)

X_train = np.array(X_train).reshape((X_train.shape[0],X_train.shape[1],1))
X_test = np.array(X_test).reshape((X_test.shape[0],X_test.shape[1],1))

y_train = pd.get_dummies(y_train)


print('Shape of y_test before: ', y_test.shape)

y_test = pd.get_dummies(y_test)

print('Shape of y_test after: ', y_test.shape)

print('Shape of X_train: ', X_train.shape)
print('Shape of y_train: ', y_train.shape)

print('Shape of y_test: ', y_test.shape)

y_test

def train_model(x_train, y_train,x_test,y_test, save_to, epoch = 2):
    strategy = tf.distribute.MirroredStrategy(devices=None)
    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
    
    with strategy.scope():
        inputs = tf.keras.Input(shape=(X_train.shape[1],1))

        #ml_model = tf.keras.layers.GRU(256, return_sequences=True)(inputs)
        ml_model = tf.keras.layers.LSTM(256, return_sequences=True)(inputs)

        flat = Flatten()(ml_model)
        outputs = Dense(4, activation='softmax')(flat)
        model = tf.keras.Model(inputs, outputs)

        #model = tf.keras.models.load_model('_best_model.h5')

        model.summary()
        tf.keras.utils.plot_model(model)

        opt_adam = keras.optimizers.Adam(learning_rate=0.001)

        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)
        mc = ModelCheckpoint(save_to + '_best_model_lstm_all_cat.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
            
        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))
            
        model.compile(optimizer=opt_adam,
                      loss=['categorical_crossentropy'],
                      metrics=['accuracy'])
        
    history = model.fit(x_train,y_train,
                        batch_size=32,
                        epochs=epoch,
                        validation_data=(x_test,y_test),
                        callbacks=[es,mc,lr_schedule])
        
    saved_model = load_model(save_to + '_best_model_lstm_all_cat.h5')
        
    return model,history

model,history = train_model(X_train, y_train,X_test, y_test, save_to= './', epoch = 40)
