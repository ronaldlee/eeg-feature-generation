{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f620c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 20:27:57.358766: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from matplotlib import pyplot as plt\n",
    "# from w207_util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800a1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (40071, 3446)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('csv/out_gameemo.csv',  sep=',')\n",
    "\n",
    "print('Shape of data: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9eb7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_4  \\\n",
      "0     0.165685    -0.852441     0.305529     0.069311    -0.820157   \n",
      "1    -1.280358    -3.529726     1.199699    -1.149790     0.738967   \n",
      "2     1.830922    -1.242559     0.019590    -0.955141     4.069830   \n",
      "3     0.726474    -0.013534     3.577241    -1.806816     0.604612   \n",
      "4    -0.338823     0.202537    -0.062122    -1.623981    -0.346947   \n",
      "\n",
      "   lag1_mean_5  lag1_mean_6  lag1_mean_7  lag1_mean_8  lag1_mean_9  ...  \\\n",
      "0     0.671192    -0.889876     0.491933    -0.284130     0.031775  ...   \n",
      "1    -4.185435    -1.864589    -0.710929     2.279327     0.201870  ...   \n",
      "2    -8.934466     0.863457     1.437240    -1.496749     0.993456  ...   \n",
      "3    -0.489957     0.936905    -2.145070    -0.171676    -0.505224  ...   \n",
      "4    -0.717592    -1.123954    -1.435684     0.115600    -1.780939  ...   \n",
      "\n",
      "   freq_669_12  freq_679_12  freq_689_12  freq_699_12  freq_709_12  \\\n",
      "0     0.010112     0.002127     0.010123     0.004699     0.003033   \n",
      "1     0.005829     0.003433     0.004874     0.005820     0.006687   \n",
      "2     0.006117     0.001497     0.001546     0.003125     0.001843   \n",
      "3     0.006868     0.007770     0.005496     0.006147     0.006058   \n",
      "4     0.015303     0.015246     0.012165     0.007093     0.005732   \n",
      "\n",
      "   freq_720_12  freq_730_12  freq_740_12  freq_750_12  Label  \n",
      "0     0.003067     0.003990     0.000784     0.004331  HA_NV  \n",
      "1     0.006809     0.004600     0.006350     0.004204  LA_PV  \n",
      "2     0.002145     0.001894     0.003016     0.001862  LA_NV  \n",
      "3     0.005706     0.003665     0.002062     0.005097  HA_PV  \n",
      "4     0.007846     0.006156     0.023414     0.009717  LA_NV  \n",
      "\n",
      "[5 rows x 3446 columns]\n",
      "Shape of data:  (40071, 3446)\n",
      "features.shape:  (40071, 3445)\n",
      "label.shape:  (40071, 1)\n",
      "Index(['lag1_mean_0', 'lag1_mean_1', 'lag1_mean_2', 'lag1_mean_3',\n",
      "       'lag1_mean_4', 'lag1_mean_5', 'lag1_mean_6', 'lag1_mean_7',\n",
      "       'lag1_mean_8', 'lag1_mean_9',\n",
      "       ...\n",
      "       'freq_669_12', 'freq_679_12', 'freq_689_12', 'freq_699_12',\n",
      "       'freq_709_12', 'freq_720_12', 'freq_730_12', 'freq_740_12',\n",
      "       'freq_750_12', 'Label'],\n",
      "      dtype='object', length=3446)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_df = dataset.copy()\n",
    "\n",
    "#HA_PV = high arousal, positive valence\n",
    "#HA_NV = high arousal, negative valence\n",
    "#LA_NV = low arousal, negative valence\n",
    "#LA_PV = low arousal, positive valance\n",
    "label_map = {1:\"HA_PV\", 2:\"HA_NV\", 3:\"LA_NV\", 4:\"LA_PV\"}\n",
    "\n",
    "init_df[\"Label\"] = init_df[\"Label\"].map(label_map)\n",
    "\n",
    "print(init_df.head())\n",
    "\n",
    "features = init_df.iloc[:, :-1]\n",
    "label = init_df.iloc[:, -1:]\n",
    "\n",
    "print('Shape of data: ', init_df.shape)\n",
    "print('features.shape: ', features.shape)\n",
    "print('label.shape: ', label.shape)\n",
    "\n",
    "init_df.head()\n",
    "print(init_df.columns)\n",
    "\n",
    "\n",
    "y = label\n",
    "X = features\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, train_size=0.6, random_state=48)\n",
    "\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_validate, y_validate, train_size=0.5, random_state=48)\n",
    "\n",
    "X_train = np.array(X_train).reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "X_validate = np.array(X_validate).reshape((X_validate.shape[0],X_test.shape[1],1))\n",
    "X_test = np.array(X_test).reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_validate = pd.get_dummies(y_validate)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fa0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train,x_validate, y_validate,\n",
    "                save_to, epoch = 2):\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "    \n",
    "    with strategy.scope():   \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(tf.keras.layers.Conv1D(\n",
    "            filters=32,                \n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            data_format='channels_last',\n",
    "            name='conv_1',\n",
    "            activation='relu'))\n",
    "        \n",
    "        model.add(tf.keras.layers.MaxPool1D(\n",
    "            pool_size=2,\n",
    "            name='pool_1'))\n",
    "        \n",
    "        # add second convolutional layer\n",
    "        model.add(tf.keras.layers.Conv1D(\n",
    "            filters=64,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            name='conv_2',\n",
    "            activation='relu'))\n",
    "\n",
    "        model.add(tf.keras.layers.MaxPool1D(\n",
    "            pool_size=2, \n",
    "            name='pool_2')\n",
    "        )\n",
    "        \n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        \n",
    "        \n",
    "        model.add(Flatten()) \n",
    "        \n",
    "        # Add Dense layer\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=1024,\n",
    "            name='fc_1', \n",
    "            activation='relu'))\n",
    "\n",
    "        # add dropout layer\n",
    "        model.add(tf.keras.layers.Dropout(\n",
    "            rate=0.5))\n",
    "        \n",
    "#         model.add(tf.keras.layers.Dense(\n",
    "#         units=512,\n",
    "#         name='fc_2', \n",
    "#         activation='relu'))\n",
    "\n",
    "#         # add dropout layer\n",
    "#         model.add(tf.keras.layers.Dropout(\n",
    "#             rate=0.5))\n",
    "\n",
    "        model.add(Dense(4))\n",
    "        model.add(Activation('softmax'))   \n",
    "\n",
    "        model.build(input_shape=(None, X_train.shape[1], 1))\n",
    "        model.summary()\n",
    "        tf.keras.utils.plot_model(model)\n",
    "\n",
    "        opt_adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + '_best_model_lstm_all_cat.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "            \n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "            \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                      loss=['categorical_crossentropy'],\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "    history = model.fit(x_train,y_train,\n",
    "                        batch_size=32,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(x_validate, y_validate),\n",
    "                        callbacks=[es,mc,lr_schedule])\n",
    "        \n",
    "    saved_model = load_model(save_to + '_best_model_lstm_all_cat.h5')\n",
    "        \n",
    "    return model,history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "913c157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv1D)              (None, 3445, 32)          128       \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling1D)        (None, 1722, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv1D)              (None, 1722, 64)          6208      \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling1D)        (None, 861, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 861, 256)          328704    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 220416)            0         \n",
      "_________________________________________________________________\n",
      "fc_1 (Dense)                 (None, 1024)              225707008 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 4100      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 226,046,148\n",
      "Trainable params: 226,046,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 20:30:00.314438: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1574\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-07-10 20:30:00.328023: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-10 20:30:00.348797: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3299990000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "INFO:tensorflow:batch_all_reduce: 11 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 11 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 20:30:04.767179: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-10 20:30:05.164087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752/752 [==============================] - ETA: 0s - loss: 3.9048 - accuracy: 0.3693"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 20:31:17.778832: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_9107\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752/752 [==============================] - 84s 103ms/step - loss: 3.9018 - accuracy: 0.3694 - val_loss: 0.7910 - val_accuracy: 0.6792\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.67919, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 2/40\n",
      "752/752 [==============================] - 75s 100ms/step - loss: 0.7692 - accuracy: 0.6841 - val_loss: 0.6700 - val_accuracy: 0.7206\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.67919 to 0.72061, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 3/40\n",
      "752/752 [==============================] - 76s 102ms/step - loss: 0.5272 - accuracy: 0.7906 - val_loss: 0.5562 - val_accuracy: 0.7725\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.72061 to 0.77252, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 4/40\n",
      "752/752 [==============================] - 77s 102ms/step - loss: 0.3698 - accuracy: 0.8553 - val_loss: 0.4508 - val_accuracy: 0.8218\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.77252 to 0.82181, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 5/40\n",
      "752/752 [==============================] - 77s 102ms/step - loss: 0.2607 - accuracy: 0.9040 - val_loss: 0.4603 - val_accuracy: 0.8272\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.82181 to 0.82718, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 6/40\n",
      "752/752 [==============================] - 77s 102ms/step - loss: 0.1784 - accuracy: 0.9348 - val_loss: 0.4177 - val_accuracy: 0.8488\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.82718 to 0.84876, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 7/40\n",
      "752/752 [==============================] - 77s 102ms/step - loss: 0.1303 - accuracy: 0.9536 - val_loss: 0.4529 - val_accuracy: 0.8496\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.84876 to 0.84964, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 8/40\n",
      "752/752 [==============================] - 77s 102ms/step - loss: 0.0901 - accuracy: 0.9678 - val_loss: 0.4506 - val_accuracy: 0.8591\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.84964 to 0.85912, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 9/40\n",
      "752/752 [==============================] - 78s 104ms/step - loss: 0.0781 - accuracy: 0.9726 - val_loss: 0.4455 - val_accuracy: 0.8585\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.85912\n",
      "Epoch 10/40\n",
      "752/752 [==============================] - 78s 104ms/step - loss: 0.0636 - accuracy: 0.9777 - val_loss: 0.4615 - val_accuracy: 0.8592\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.85912 to 0.85925, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 11/40\n",
      "752/752 [==============================] - 78s 103ms/step - loss: 0.0533 - accuracy: 0.9821 - val_loss: 0.4534 - val_accuracy: 0.8679\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.85925 to 0.86786, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 12/40\n",
      "752/752 [==============================] - 78s 103ms/step - loss: 0.0402 - accuracy: 0.9849 - val_loss: 0.4678 - val_accuracy: 0.8690\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.86786 to 0.86898, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 13/40\n",
      "752/752 [==============================] - 78s 104ms/step - loss: 0.0305 - accuracy: 0.9894 - val_loss: 0.4584 - val_accuracy: 0.8776\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.86898 to 0.87759, saving model to ./_best_model_lstm_all_cat.h5\n",
      "Epoch 14/40\n",
      "752/752 [==============================] - 78s 103ms/step - loss: 0.0236 - accuracy: 0.9915 - val_loss: 0.4619 - val_accuracy: 0.8771\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.87759\n",
      "Epoch 15/40\n",
      "752/752 [==============================] - 78s 104ms/step - loss: 0.0247 - accuracy: 0.9921 - val_loss: 0.5203 - val_accuracy: 0.8712\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.87759\n",
      "Epoch 16/40\n",
      "752/752 [==============================] - 78s 104ms/step - loss: 0.0216 - accuracy: 0.9929 - val_loss: 0.5047 - val_accuracy: 0.8706\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.87759\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model,history = train_model(X_train, y_train, X_validate, y_validate, save_to= './', epoch = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_accuracy_history(history):\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['accuracy'] + 1)))\n",
    "  plt.plot(history['accuracy'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_accuracy'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't plot, big image data to commit to git.\n",
    "# history_data = pd.DataFrame(history.history)\n",
    "# plot_loss_history(history_data)\n",
    "# plot_accuracy_history(history_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b23cd0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/251 [..............................] - ETA: 5s - loss: 0.6066 - accuracy: 0.8984 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 20:58:02.442110: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_46331\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 5s 20ms/step - loss: 0.5144 - accuracy: 0.8784\n",
      "\n",
      "Test Acc. 87.84%\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_results = model.evaluate(X_test, y_test)\n",
    "print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validate only, no test..\n",
    "#LSTM alone: loss: 0.0205 - accuracy: 0.9949 - val_loss: 1.1336 - val_accuracy: 0.7971\n",
    "#1 Conv1D (32 filters) -> LSTM: loss: 5.6893e-04 - accuracy: 1.0000 - val_loss: 0.7952 - val_accuracy: 0.8299\n",
    "#2 Conv1D(32/64 filters)->LSTM: loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.7513 - val_accuracy: 0.8368\n",
    "#3 #2 + 1 Dense Layer (1024)  : loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.4540 - val_accuracy: 0.9013\n",
    "#3 #3 + 1 Dense Layer (512)   : loss: 1.3862 - accuracy: 0.2541 - val_loss: 1.3862 - val_accuracy: 0.2576\n",
    "\n",
    "# train/validate/test\n",
    "#3: Epoch 16/40: loss: 0.0216 - accuracy: 0.9929 - val_loss: 0.5047 - val_accuracy: 0.8706, Test Acc. 87.84%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
